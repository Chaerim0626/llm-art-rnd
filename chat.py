import streamlit as st
from langchain.schema import HumanMessage, AIMessage
from langchain_community.vectorstores import FAISS
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from langchain import HuggingFacePipeline
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableMap
import re
import uuid

# Streamlit UI ì„¤ì •
@st.cache_resource

# EXAONE ëª¨ë¸ ì„¤ì •
@st.cache_resource
def load_pipeline(model_id):
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_use_double_quant=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=quantization_config,
        device_map="cuda",  # CUDAì—ì„œ ìë™ ë°°ì¹˜
        trust_remote_code=True
    )
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        do_sample=True,
        temperature=0.1,
        top_k=50,
        repetition_penalty=1.05
    )
    return HuggingFacePipeline(pipeline=pipe)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
def load_prompt_template():
    template = '''
<|system|>
  You are a friendly chatbot specializing in artworks. 
  Answer questions strictly based on the information provided in the document (context). 
  If the requested information is not found in the document, respond with "The document does not contain this information." 
  Provide detailed and comprehensive answers, always include the artwork ID, and ensure all answers are written in polite and grammatically correct Korean.
  Your tone should always be friendly, formal, and respectful.
  If the question is unclear or incomplete, politely ask for clarification in Korean.
  If possible, provide related details from the context that may enrich the user's understanding of the artwork.

<|context|>
{context}

<|user|>
Question: {question}

<|assistant|>
'''
    return ChatPromptTemplate.from_template(template)

def extract_answer(text):
    """
    ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ 'Answer:' ë˜ëŠ” íŠ¹ì • íŒ¨í„´ ì´í›„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ.
    - text: ëª¨ë¸ì—ì„œ ìƒì„±ëœ í…ìŠ¤íŠ¸
    """
    match = re.search(r"<\|assistant\|>\s*(.*)", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return "ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def process_input(llm, retriever, prompt, user_input):
    """
    ì…ë ¥ëœ ì§ˆë¬¸ê³¼ ë¬¸ë§¥(Context)ì„ ê²°í•©í•˜ì—¬ ëª¨ë¸ì— ì „ë‹¬.
    - llm: ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ê°ì²´
    - retriever: ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê°ì²´
    - prompt: ChatPromptTemplate ê°ì²´
    - user_input: ì‚¬ìš©ì ì…ë ¥ ì§ˆë¬¸
    """
    # ë¬¸ë§¥(Context) ìƒì„±
    context = retriever.invoke(user_input)
    context_text = " ".join(doc.page_content for doc in context) if context else "ê´€ë ¨ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    input_text_with_context = prompt.format(context=context_text, question=user_input)
    
    # ëª¨ë¸ ì‘ë‹µ ìƒì„±
    response = llm.pipeline(input_text_with_context)
    
    # ì‘ë‹µì—ì„œ <|assistant|> ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    return extract_answer(response[0])


# MarkdownOutputParser ì •ì˜
class MarkdownOutputParser:
    def __call__(self, llm_output):
        match = re.search(r"<\|assistant\|>\s*(.*)", llm_output, re.DOTALL)
        if match:
            extracted_text = match.group(1).strip()
            return f"### ëª¨ë¸ ê²°ê³¼\n\n{extracted_text}\n\n"
        else:
            return f"### ëª¨ë¸ ê²°ê³¼\n\n{llm_output.strip()}\n\n"

# ì„¸ì…˜ ì„¤ì •
st.set_page_config(page_title="Artwork Chatbot")
st.title('ğŸ¤– ë¯¸ìˆ ì‘í’ˆ QA ì±—ë´‡')

# ì„¸ì…˜ ID ê´€ë¦¬
session_id = st.session_state.get('session_id', None)
if not session_id:
    session_id = str(uuid.uuid4())
    st.session_state['session_id'] = session_id

# FAISS ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
faiss_path = "./faiss_artworks"
embedding_model = SentenceTransformer("nlpai-lab/KURE-v1")
with st.spinner("Loading FAISS database..."):
    faiss_db = FAISS.load_local(
        folder_path=faiss_path,
        embeddings=embedding_model,
        allow_dangerous_deserialization=True
    )
    st.success("FAISS database loaded successfully!")

# embedding_function ë””ë²„ê¹…
def debug_embedding_function(text):
    print(f"Debug: embedding_function input type: {type(text)}")
    print(f"Debug: embedding_function input: {text}")
    return embedding_model.encode(text)

faiss_db.embedding_function = debug_embedding_function

# LLM ë° í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™”
llm = load_pipeline("LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct")
prompt = load_prompt_template()

retriever = faiss_db.as_retriever(
    search_kwargs={
        "k": 5,                # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        "fetch_k": 20,         # ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        "mmr": True,           # MMR í™œì„±í™”
        "mmr_beta": 0.5        # ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„± ê°„ ê· í˜•
    }
)

# ì²´ì¸ êµ¬ì„±
chain = (
    RunnableMap({
        "context": lambda query: retriever.get_relevant_documents(query["question"]),  # Retriever í˜¸ì¶œ
        "question": RunnablePassthrough()
    })
    | (lambda x: {
        "context": "\n\n".join([doc.page_content for doc in x["context"]]),  # ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        "question": x["question"]
    })
    | prompt  # Prompt Templateì— ì „ë‹¬
    | llm     # LLM í˜¸ì¶œ
    | MarkdownOutputParser()  # Markdown í¬ë§·ìœ¼ë¡œ ì¶œë ¥
)


# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
st.subheader("ëŒ€í™” ê¸°ë¡")
if 'messages' not in st.session_state:
    st.session_state['messages'] = []

for message in st.session_state['messages']:
    if isinstance(message, HumanMessage):
        st.write(f"**ì‚¬ìš©ì:** {message.content}")
    elif isinstance(message, AIMessage):
        st.write(f"**ì´ì „ ë‹µë³€:** {message.content}")

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
with st.form('Question'):
    user_input = st.text_area('ì§ˆë¬¸ ë‚´ìš©:', 'ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.')
    submitted = st.form_submit_button('ë³´ë‚´ê¸°')

    if submitted:
        with st.spinner("Processing your query..."):
            try:
                # ì²´ì¸ ì‹¤í–‰
                response = chain.invoke({"question": user_input})

                # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
                st.session_state['messages'].append(HumanMessage(content=user_input))
                st.session_state['messages'].append(AIMessage(content=response))

                # í˜„ì¬ ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶œë ¥
                st.markdown("---")
                st.markdown("### í˜„ì¬ ëŒ€í™”")
                st.write(f"**ì§ˆë¬¸:** {user_input}")
                st.markdown(f"**ë‹µë³€:**\n{response}")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
