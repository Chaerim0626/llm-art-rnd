import streamlit as st
from langchain.schema import HumanMessage, AIMessage
from langchain_community.vectorstores import FAISS
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from langchain import HuggingFacePipeline
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableMap
import uuid, time, os, re
from collections import OrderedDict

# ì„¸ì…˜ ID íŒŒì¼ ê²½ë¡œ ì„¤ì •
SESSION_ID_FILE = "session_id.txt"

# ì„¸ì…˜ ID ë¶ˆëŸ¬ì˜¤ê¸° ë˜ëŠ” ìƒì„±í•˜ê¸°
def get_or_create_session_id():
    if os.path.exists(SESSION_ID_FILE):
        with open(SESSION_ID_FILE, "r") as f:
            session_id = f.read().strip()
            if session_id:
                return session_id
    # íŒŒì¼ì´ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    session_id = str(uuid.uuid4())
    with open(SESSION_ID_FILE, "w") as f:
        f.write(session_id)
    return session_id


# ì„¸ì…˜ ê´€ë¦¬ìš© In-Memory Store
if "messages" not in st.session_state:
    st.session_state["messages"] = OrderedDict()

# ì„¸ì…˜ ID ì„¤ì •
if "session_id" not in st.session_state:
    st.session_state["session_id"] = get_or_create_session_id()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Artwork Chatbot", layout="wide")
st.title('ğŸ¤– ë¯¸ìˆ ì‘í’ˆ QA ì±—ë´‡')


session_id = st.session_state["session_id"]

# ë””ë²„ê¹…: í˜„ì¬ ì„¸ì…˜ ìƒíƒœ ì¶œë ¥
st.write("[DEBUG] Current session_state:")
st.write(st.session_state)

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
st.subheader("ëŒ€í™” ê¸°ë¡")
for message_id, message_data in st.session_state["messages"].items():
    if message_data["type"] == "user":
        st.write(f"**ì‚¬ìš©ì ({message_id}):** {message_data['content']}")
    elif message_data["type"] == "ai":
        st.write(f"**AI ({message_id}):** {message_data['content']}")

@st.cache_resource
# EXAONE ëª¨ë¸ ì„¤ì •
def load_pipeline(model_id):
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_use_double_quant=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=quantization_config,
        device_map="cuda",
        trust_remote_code=True,
    )
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        do_sample=True,
        temperature=0.1,
        top_k=50,
        repetition_penalty=1.05,
    )
    return HuggingFacePipeline(pipeline=pipe)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
def load_prompt_template():
    template = '''
    <|system|>
    You are a friendly chatbot specializing in artworks. 
    Answer questions strictly based on the information provided in the document (context). 
    If the requested information is not found in the document, respond with "The document does not contain this information." 
    Provide detailed and comprehensive answers, always include the artwork number, and ensure all answers are written in Korean. 
    All answers should be formatted using beautiful Markdown syntax to make the response visually appealing and easy to read. 
    Use headings, bullet points, and bold or italic text where appropriate to enrich the response.

    <|context|>
    {context}

    <|user|>
    Question: {question}

    <|assistant|>
    '''

    return ChatPromptTemplate.from_template(template)




# ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ ì¶”ì¶œ
def extract_answer(text):
    match = re.search(r"<\|assistant\|>\s*(.*)", text, re.DOTALL)
    if match:
        extracted_text = match.group(1).strip()
        # ë¶ˆí•„ìš”í•œ ë“¤ì—¬ì“°ê¸° ì œê±°
        cleaned_text = re.sub(r"^\s{2,}", "", extracted_text, flags=re.MULTILINE)
        return f"### ëª¨ë¸ ê²°ê³¼\n\n{cleaned_text}\n"
    else:
        return f"### ëª¨ë¸ ê²°ê³¼\n\n{text.strip()}\n"




# ëŒ€í™” ê¸°ë¡ ìƒì„±
def generate_chat_history():
    chat_history = []
    for message_id, message_data in st.session_state["messages"].items():
        if message_data["type"] == "user":
            chat_history.append(f"User: {message_data['content']}")
        elif message_data["type"] == "ai":
            chat_history.append(f"AI: {message_data['content']}")
    return "\n".join(chat_history)

# FAISS ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
faiss_path = "./faiss_artworks_0114"
embedding_model = SentenceTransformer("nlpai-lab/KURE-v1")
with st.spinner("Loading FAISS database..."):
    faiss_db = FAISS.load_local(
        folder_path=faiss_path,
        embeddings=embedding_model,
        allow_dangerous_deserialization=True
    )
    st.success("FAISS database loaded successfully!")

faiss_db.embedding_function = lambda text: embedding_model.encode(text)

# LLM ë° í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™”
llm = load_pipeline("LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct")
prompt = load_prompt_template()

retriever = faiss_db.as_retriever(
    search_kwargs={
        "k": 5,
        "fetch_k": 15,
        "mmr": True,
        "mmr_beta": 0.8
    }
)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
with st.form("Question"):
    user_input = st.text_area("ì§ˆë¬¸ ë‚´ìš©:", "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    submitted = st.form_submit_button("ë³´ë‚´ê¸°")

if submitted:
    try:
        # ë©”ì‹œì§€ ID ìƒì„±
        user_message_id = str(uuid.uuid4())
        ai_message_id = str(uuid.uuid4())

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        st.session_state["messages"][user_message_id] = {"type": "user", "content": user_input}

        # ë¬¸ë§¥ ê²€ìƒ‰ (ìºì‹± ì ìš©)
        start_time = time.time()
        context_documents = retriever.get_relevant_documents(user_input)
        context = "\n\n".join([doc.page_content for doc in context_documents])
        chat_history = generate_chat_history()

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        formatted_prompt = prompt.format(
            chat_history=chat_history,
            context=context,
            question=user_input
        )

        # ëª¨ë¸ í˜¸ì¶œ 
        response = llm.invoke(formatted_prompt)
        response_text = extract_answer(response["generated_text"] if isinstance(response, dict) else response)
        print(type(response_text))
        # AI ë©”ì‹œì§€ ì €ì¥
        st.session_state["messages"][ai_message_id] = {"type": "ai", "content": response_text}

        # í˜„ì¬ ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶œë ¥
        st.markdown("---")
        st.markdown("### í˜„ì¬ ëŒ€í™”")
        st.markdown(f"**ì§ˆë¬¸:** {user_input}")
        st.markdown(f"**ë‹µë³€:**\n\n")
        st.markdown(response_text, unsafe_allow_html=False)
        print(response_text)
        # ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        end_time = time.time()
        st.write(f"ì‘ë‹µ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")

    except Exception as e:
        # ì˜¤ë¥˜ ì²˜ë¦¬
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
