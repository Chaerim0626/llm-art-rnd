import streamlit as st
from langchain.schema import HumanMessage, AIMessage
from langchain_community.vectorstores import FAISS
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from langchain import HuggingFacePipeline
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableMap
import uuid, time, os, re
from collections import OrderedDict


# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Artwork Chatbot", layout="wide")
st.title('ğŸ¤– ë¯¸ìˆ ì‘í’ˆ QA ì±—ë´‡')

@st.cache_resource
# EXAONE ëª¨ë¸ ì„¤ì •
def load_pipeline(model_id):
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_use_double_quant=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=quantization_config,
        device_map="cuda",
        trust_remote_code=True,
    )
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        do_sample=True,
        temperature=0.1,
        top_k=50,
        repetition_penalty=1.05,
    )
    return HuggingFacePipeline(pipeline=pipe)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
def load_prompt_template():
    template = '''
    <|system|>
    You are a friendly chatbot specializing in artworks and general conversations.
    Your primary role is to answer questions strictly based on the information provided in the document (context). 
    If the requested information is not found in the document, respond with:
    "The document does not contain this information." in Korean.

    However, if the question is a general conversation or does not relate to the document, you should respond naturally as a conversational chatbot. 
    You can talk about art history, artists, exhibitions, and general topics such as daily life, technology, and culture. 
    Maintain a friendly and engaging tone, ensuring all responses are written in **Korean**.
    Always use **beautiful Markdown formatting** (headings, bullet points, bold or italic text) to enhance readability.

    <|context|>
    {context}

    <|user|>
    Question: {question}

    <|assistant|>
    '''

    return ChatPromptTemplate.from_template(template)


# ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ ì¶”ì¶œ
def extract_answer(text):
    match = re.search(r"<\|assistant\|>\s*(.*)", text, re.DOTALL)
    if match:
        extracted_text = match.group(1).strip()
        # ë¶ˆí•„ìš”í•œ ë“¤ì—¬ì“°ê¸° ì œê±°
        cleaned_text = re.sub(r"^\s{2,}", "", extracted_text, flags=re.MULTILINE)
        return f"### ëª¨ë¸ ê²°ê³¼\n\n{cleaned_text}\n"
    else:
        return f"### ëª¨ë¸ ê²°ê³¼\n\n{text.strip()}\n"


# ëŒ€í™” ê¸°ë¡ ìƒì„±
def generate_chat_history():
    chat_history = []
    for message_id, message_data in st.session_state["messages"].items():
        if message_data["type"] == "user":
            chat_history.append(f"User: {message_data['content']}")
        elif message_data["type"] == "ai":
            chat_history.append(f"AI: {message_data['content']}")
    return "\n".join(chat_history)


# FAISS ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
faiss_path = "./faiss_artworks_0114"
embedding_model = SentenceTransformer("nlpai-lab/KURE-v1")
with st.spinner("Loading FAISS database..."):
    faiss_db = FAISS.load_local(
        folder_path=faiss_path,
        embeddings=embedding_model,
        allow_dangerous_deserialization=True
    )
    st.success("FAISS database loaded successfully!")

faiss_db.embedding_function = lambda text: embedding_model.encode(text)


# LLM ë° í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™”
llm = load_pipeline("LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct")
prompt = load_prompt_template()

retriever = faiss_db.as_retriever(
    search_kwargs={
        "k": 5,                # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
        "fetch_k": 20,         # ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        "mmr": True,           # MMR í™œì„±í™”
        "mmr_beta": 0.8      # ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„± ê°„ ê· í˜•
    }
)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
with st.form("Question"):
    user_input = st.text_area("ì§ˆë¬¸ ë‚´ìš©:", "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    submitted = st.form_submit_button("ë³´ë‚´ê¸°")

if submitted:
    try:
        # ë©”ì‹œì§€ ID ìƒì„±
        user_message_id = str(uuid.uuid4())
        ai_message_id = str(uuid.uuid4())

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        st.session_state["messages"][user_message_id] = {"type": "user", "content": user_input}

        # ë¬¸ë§¥ ê²€ìƒ‰ (ìºì‹± ì ìš©)
        start_time = time.time()
        context_documents = retriever.get_relevant_documents(user_input)
        context = "\n\n".join([doc.page_content for doc in context_documents])
        chat_history = generate_chat_history()

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        formatted_prompt = prompt.format(
            chat_history=chat_history,
            context=context,
            question=user_input
        )

        # ëª¨ë¸ í˜¸ì¶œ 
        response = llm.invoke(formatted_prompt)
        response_text = extract_answer(response["generated_text"] if isinstance(response, dict) else response)
        print(type(response_text))
        # AI ë©”ì‹œì§€ ì €ì¥
        st.session_state["messages"][ai_message_id] = {"type": "ai", "content": response_text}

        # í˜„ì¬ ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶œë ¥
        st.markdown("---")
        st.markdown("### í˜„ì¬ ëŒ€í™”")
        st.markdown(f"**ì§ˆë¬¸:** {user_input}")
        st.markdown(response_text, unsafe_allow_html=False)
        print(response_text)
        # ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        end_time = time.time()
        st.write(f"ì‘ë‹µ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")

    except Exception as e:
        # ì˜¤ë¥˜ ì²˜ë¦¬
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
