{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e06783-f083-424c-9ace-51f3c3bf5e00",
   "metadata": {},
   "source": [
    "### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacafc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt  # 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f425499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # PyTorch 버전\n",
    "print(torch.version.cuda)  # PyTorch가 사용하는 CUDA 버전\n",
    "print(torch.cuda.is_available())  # GPU 사용 가능 여부\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d49d96-9721-43a6-8482-9158d25c93fe",
   "metadata": {},
   "source": [
    "### 2. 문서 split 및 Chroma를 활용한 vector store 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d82f23-0f73-4879-8a69-46d5237a47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chae/env_0217/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로그인 상태입니다. 사용자: chaeeee\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"로그인 상태입니다. 사용자: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(\"로그인되지 않았거나 토큰이 유효하지 않습니다.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ea6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b8cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Documents: 100%|█████████| 11479/11479 [00:00<00:00, 71299.71entry/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. JSON 파일 경로 설정\n",
    "json_path = \"./json_data.json\"  # 단일 JSON 파일 경로\n",
    "\n",
    "# 2. JSON 데이터 불러오기\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# 3. {}를 기준으로 JSON 데이터 분할 및 Document 객체 생성\n",
    "documents = []\n",
    "\n",
    "for data in tqdm(all_data, desc=\"Generating Documents\", unit=\"entry\", ncols=80):\n",
    "    metadata = {\n",
    "        \"title\": data.get('title', 'N/A'),\n",
    "        \"artist\": data.get('artist', 'N/A'),\n",
    "        \"year\": data.get('year', 'N/A'),\n",
    "        \"read_count\": data.get('read_count', 0),\n",
    "        \"collection\": data.get('collection')\n",
    "    }\n",
    "\n",
    "    # JSON 데이터의 각 항목을 Document 객체로 변환\n",
    "    doc_content = json.dumps(data, ensure_ascii=False, indent=4)\n",
    "    documents.append(Document(\n",
    "        page_content=doc_content.strip(),\n",
    "        metadata=metadata\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a08bbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11479"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a1a70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': '팔괘호', 'artist': '한기석', 'year': '1960', 'read_count': 175, 'collection': '국립현대미술관'}, page_content='{\\n    \"title\": \"팔괘호\",\\n    \"title_eng\": \"Palgwae Vase\",\\n    \"artist\": \"한기석\",\\n    \"artist_eng\": \"HAN Kisuk\",\\n    \"artwork_number\": 2,\\n    \"year\": \"1960\",\\n    \"size\": \"250×127\",\\n    \"materials\": \"캔버스, 종이에 유화 물감\",\\n    \"category\": \"회화 II\",\\n    \"description\": \"한농(韓農) 한기석(1930-2011)은 표면 묘사에 많은 관심을 가진 작가이다.그는 모나고 약간 무게가 있는 듯이 보이는 항아리와 화병을 계속 그렸는데, 항아리에 대한 사람들의 일반적 향수 이미지는 그의 작품에서 친근감을 불어넣어 준다. 또한 완벽한 균형을 이루면서 노련하게 표현되어 다양한 색의 조화를 지닌 도자기의 미를 느낄 수 있다.\",\\n    \"read_count\": 175,\\n    \"collection\": \"국립현대미술관\"\\n}')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c102d292-48e8-4eeb-9280-79dc003ed796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특정 Document 객체의 텍스트 길이 확인\n",
    "len(documents[11001].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcec14a4-2333-4381-8152-dc30d011ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800자를 초과하는 Document 개수: 6971\n"
     ]
    }
   ],
   "source": [
    "# 800자를 초과하는 Document 개수 세기\n",
    "over_800_count = sum(1 for doc in documents if len(doc.page_content) > 800)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"800자를 초과하는 Document 개수: {over_800_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1ef186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chae/env_0217/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "You try to use a model that was created with version 3.3.1, however, your version is 3.2.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss  # FAISS 라이브러리 필요\n",
    "\n",
    "# 1. 임베딩 초기화\n",
    "embedding_model = SentenceTransformer(\"nlpai-lab/KURE-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce036d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 문서 데이터와 메타데이터 분리\n",
    "texts = [doc.page_content for doc in documents]  # 문서 텍스트\n",
    "metadatas = [doc.metadata for doc in documents]  # 문서 메타데이터\n",
    "\n",
    "# 3. 문서 임베딩 생성\n",
    "embeddings = embedding_model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23144ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 4. FAISS 인덱스 생성\n",
    "embedding_dim = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "faiss_index.add(embeddings.astype(np.float32))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd078ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 데이터베이스가 성공적으로 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "\n",
    "# 6. index_to_docstore_id도 str로!\n",
    "index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "\n",
    "# 7. FAISS 벡터스토어 생성\n",
    "def embed_query(text):\n",
    "    return embedding_model.encode([text])[0]\n",
    "\n",
    "faiss_db = FAISS(\n",
    "    index=faiss_index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    "    embedding_function=embed_query\n",
    ")\n",
    "\n",
    "# 7. FAISS 데이터베이스 저장\n",
    "faiss_db.save_local(\"./faiss_artworks_0324_json\")\n",
    "print(\"FAISS 데이터베이스가 성공적으로 저장되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c40e5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 텍스트: {\n",
      "    \"title\": \"설경(雪景)\",\n",
      "    \"title_eng\": \"Snowy Landscape\",\n",
      "    \"artist\": \"박승무\",\n",
      "    \"artist_eng\": \"PARK Seungmoo\",\n",
      "    \"artwork_number\": 9402,\n",
      "    \"year\": \"1965\",\n",
      "    \"size\": \"157×157\",\n",
      "    \"materials\": \"종이에 먹, 색\",\n",
      "    \"category\": \"회화 I\",\n",
      "    \"description\": \"심향(深香) 박승무(朴勝武, 1893-1980)는 1913년부터 서화미술회의 안중식(安中植, 1861-1919)과 조석진(趙錫晋, 1853-1920) 문하에서 전통화법을 배웠으며, 졸업한 이듬해 중국 상하이로 향했으나 1919년 3·1 운동 이후 귀국했다. 《조선미술전람회》와 《서화협회전람회》에 출품하며 활동을 시작하였고, 광복 이후에는 대전에 정착하여 작업을 지속했다. 1957년 제1회 충청남도 문화상 미술부문 수상자로 선정되었다.<설경>은 눈 온 후의 산촌 풍경을 담은 산수화로, 박승무가 즐겨 그렸던 주제 중 하나이다. 작가는 화면 왼쪽에 커다란 산을 배치하고, 오른쪽으로는 공간을 시원하게 열어주는 구도를 취했다. 근경의 지팡이를 든 노인과 아이는 박승무의 산수화에 자주 등장하는 인물상으로, 점경인물과 가옥이 어우러져 정감있게 표현되었다.\",\n",
      "    \"read_count\": 59,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "문서 메타데이터: {'title': '설경(雪景)', 'artist': '박승무', 'year': '1965', 'read_count': 59, 'collection': '국립현대미술관'}\n",
      "=------------------------------------------\n",
      "문서 텍스트: {\n",
      "    \"title\": \"설경\",\n",
      "    \"title_eng\": \"Snowscape\",\n",
      "    \"artist\": \"박승무\",\n",
      "    \"artist_eng\": \"PARK Seungmoo\",\n",
      "    \"artwork_number\": 3372,\n",
      "    \"year\": \"1974\",\n",
      "    \"size\": \"65×255\",\n",
      "    \"materials\": \"종이에 먹, 색\",\n",
      "    \"category\": \"회화 I\",\n",
      "    \"description\": \"심향(深香)박승무(1893-1980)는 설경을 위주로 작품을 해왔다. 눈 덮인 초가집과 나무들의 사실적인 표현과는 달리 배경을 이루는 산의 묘사는 구도에서 도식적이리 만큼 빈번히 등장한다. 먼 산의 윤곽선 및 나무들의 부드럽고 정감 짙은 풍정미, 잔 점과 선의 표현적 특질 등이 그의 독자적인 양식으로 간주된다.<설경>(1974)이 제작된 무렵 박승무는 설경의 명수로서 유명하였으며 종축보다는 단독 회화성이 더 짙은 횡축그림을 많이 그렸다. 그는 자연이 지니고 있는 가장 원형적인 것을 찾아내고 자연의 원형을 자기화함으로써 독특한 양식을 이루어 내었다. 이 작품은 한국의 야산을 대상으로 하여 향토주의 수묵화를 추구하고 있다. 화면 상단의 하늘과 하단의 강물을 중묵의 음영으로 처리하여 설경의 효과를 더욱 실감나게 하였다.\",\n",
      "    \"read_count\": 84,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "문서 메타데이터: {'title': '설경', 'artist': '박승무', 'year': '1974', 'read_count': 84, 'collection': '국립현대미술관'}\n",
      "=------------------------------------------\n",
      "문서 텍스트: {\n",
      "    \"title\": \"설경\",\n",
      "    \"title_eng\": \"Snowy Landscape\",\n",
      "    \"artist\": \"박승무\",\n",
      "    \"artist_eng\": \"PARK Seungmoo\",\n",
      "    \"artwork_number\": 289,\n",
      "    \"year\": \"1959\",\n",
      "    \"size\": \"137×137\",\n",
      "    \"materials\": \"종이에 먹, 색\",\n",
      "    \"category\": \"회화 I\",\n",
      "    \"description\": \"심향(深香) 박승무(朴勝武, 1893-1980)는 1913년에 서화미술회(書畫美術會)에 들어가 조석진과 안중식에게 동양화를 사사했다. 1917년에 상해로 건너가 3년간 전통 화법(畫法)을 연구하며 견문을 넓혔다. 광복 후 대한민국미술전람회 초대 작가로 제안을 받았으나 모두 거절하고 대전에 정착하여 오로지 창작에만 집중했다. 긴 은둔 생활 끝에 1971년에 서울신문사가 기획한 전시회에 출품했고, 1972년과 1976년에는 국립현대미술관 단체전에 참여하는 등 활발한 활동을 했다.박승무의 초기작은 사실적인 향토 풍경이 주를 이루었으나 1930년대 후반부터 전통 산수화를 제작하며 독자적인 양식을 형성해 나갔으며 1940년대에 이르러 안정된 기법의 전통 산수 양식을 정립했다. 그는 남종화풍을 따랐으나 시각과 준법*에 있어서는 엄격한 형식미를 추구했으며 농묵의 변화를 배제하고 대상을 관념적으로 파악했다.<설경>은 작가가 중년 이후 관념화된 작품 경향을 보여 주는 작품으로 눈 덮인 산과 언덕, 나무 등은 모두 전통적인 남종화풍으로 그려졌다. 그러나 먼 산의 윤곽선, 부드럽게 표현된 수목, 태점 등은 작가 특유의 온화하고 소박한 작풍을 보여 준다.*준법: 준법은 산수화를 그릴 때 산, 바위 등의 입체감, 양감, 질감, 명암 등을 나타내기 위해표면을 처리하는 기법을 가리킨다.[화제 풀이]雪晴時己亥菊秋 深香散人눈이 갠 후기해년(1959) 국추(菊秋, 음력 9월) 심향산인(深香散人)\",\n",
      "    \"read_count\": 93,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "문서 메타데이터: {'title': '설경', 'artist': '박승무', 'year': '1959', 'read_count': 93, 'collection': '국립현대미술관'}\n",
      "=------------------------------------------\n",
      "문서 텍스트: {\n",
      "    \"title\": \"산수\",\n",
      "    \"title_eng\": \"Landscape\",\n",
      "    \"artist\": \"박승무\",\n",
      "    \"artist_eng\": \"PARK Seungmoo\",\n",
      "    \"artwork_number\": 10838,\n",
      "    \"year\": \"N/A\",\n",
      "    \"size\": \"130×38\",\n",
      "    \"materials\": \"비단에 먹, 색\",\n",
      "    \"category\": \"회화 I\",\n",
      "    \"description\": \"심향(深香) 박승무(朴勝武, 1893-1980)는 1913년에 서화미술회에 들어가 안중식과 조석진을 사사했다. 1917년에는 중국 상하이로 건너가 3년간 전통화법을 연구하며 견문을 넓혔다. 1940년 조선미술관 주최하에 경성 부민관(府民館)에서 열린 《십대가산수풍경화전(十大家山水風景畵展)》*에 초대되어 한국 동양화 십대가 중의 하나로 불렸다. 광복 후에는 《대한민국미술전람회》에 초대작가를 제안받기도 하였으나 거절하고 대전에 정착하여 오로지 작업에만 몰두하였다. 긴 은둔 생활 끝에 1971년 서울신문사에서 기획한 《동양화 6대가전》에 참여하면서부터 활발한 활동을 했다.박승무의 초기작은 현장 사생을 기반으로 한 사실적인 향토 풍경이 주를 이루었으나 1930년대 후반부터는 전통 산수화를 제작하였다. 그는 남종화풍을 따랐으며, 시각과 준법에 있어서는 엄격한 형식미를 추구했다.<산수>는 산수풍경을 그린 것으로, 화면 위 쪽 여백에는 제발문을 적음으로써 시서화를 일치시켰다. 피마준**을 사용하여 산의 겉면을 표현했으며, 안개 낀 산의 표현 등에서 남종화풍의 산수임을 알 수 있다. 박승무는 설경산수에 능했고 설경을 주로 그렸으나, 이 작품은 여름날의 풍경을 그린 듯하다. 산에서 시원스레 내려오는 계곡물과 그 앞에 앉아 쉬고 있는 인물은 화면에 생동감을 불어 넣는다.*주최 측의 엄격한 심사를 거쳐 선발한 열 명의 화가들로, 박승무, 김은호, 고희동, 허백련, 이상범, 노수현, 변관식, 이용우, 최우석, 이한복 등이다.**피마준(披麻皴)은 동양 산수화의 준법 중 하나로, 산의 겉면을 표현하는데 사용하며 삼베의 올을 풀어 놓은 듯 구불거리는 선을 여러 번 그어 입체적으로 보이게 한다.\",\n",
      "    \"read_count\": 1091,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "문서 메타데이터: {'title': '산수', 'artist': '박승무', 'year': 'N/A', 'read_count': 1091, 'collection': '국립현대미술관'}\n",
      "=------------------------------------------\n",
      "문서 텍스트: {\n",
      "    \"title\": \"연해풍경\",\n",
      "    \"title_eng\": \"Seascape\",\n",
      "    \"artist\": \"박승무\",\n",
      "    \"artist_eng\": \"PARK Seungmoo\",\n",
      "    \"artwork_number\": 8804,\n",
      "    \"year\": \"1953\",\n",
      "    \"size\": \"40x61\",\n",
      "    \"materials\": \"종이에 먹, 색\",\n",
      "    \"category\": \"회화 I\",\n",
      "    \"description\": \"심향(深香) 박승무(朴勝武, 1893-1980)는 1913년부터 서화미술회의 안중식(安中植, 1861-1919)과 조석진(趙錫晋, 1853-1920) 문하에서 전통화법을 배웠으며, 졸업한 이듬해 중국 상하이로 향했으나 1919년 3·1 운동 이후 귀국했다. 《조선미술전람회》와 《서화협회전람회》에 출품하며 활동을 시작하였고, 광복 이후에는 대전에 정착하여 작업을 지속했다. 1957년 제1회 충청남도 문화상 미술부문 수상자로 선정되었다.<연해풍경>은 목포 바닷가의 풍경을 담은 작품으로, 생생한 실경의 느낌을 전달하는 산수화이다. 화면 왼쪽에는 산과 마을을, 오른쪽에는 평온한 바다가 보인다. 바위와 지면의 표현에는 짧은 선을 여러 번 겹치고 미점(米点)을 활용했다. 노를 젓는 사공과 배를 탄 인물들, 옹기종기 모여있는 초가집을 통해 정감 있는 풍치를 담아내고 있다.\",\n",
      "    \"read_count\": 141,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "문서 메타데이터: {'title': '연해풍경', 'artist': '박승무', 'year': '1953', 'read_count': 141, 'collection': '국립현대미술관'}\n",
      "=------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 9. 검색 테스트\n",
    "query = \"박승무의 설경\"\n",
    "results = faiss_db.similarity_search(query, k=5)\n",
    "\n",
    "# 10. 검색 결과 출력\n",
    "for result in results:\n",
    "    print(\"문서 텍스트:\", result.page_content)\n",
    "    print(\"문서 메타데이터:\", result.metadata)\n",
    "    print('=------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed8b5f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 index.pkl 내용: <class 'tuple'>, 길이: 2\n",
      "📌 데이터 내부 타입: [<class 'langchain_community.docstore.in_memory.InMemoryDocstore'>, <class 'dict'>]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "index_pkl_path = \"./faiss_artworks_0324_json/index.pkl\"\n",
    "\n",
    "# index.pkl 파일을 직접 열어서 확인\n",
    "with open(index_pkl_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"📌 index.pkl 내용: {type(data)}, 길이: {len(data)}\")\n",
    "\n",
    "if isinstance(data, tuple):\n",
    "    print(f\"📌 데이터 내부 타입: {[type(item) for item in data]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6a00995-3f64-49d2-8b0f-033e8b640085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss_db 로드 성공\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# 기존 DB 로드 \n",
    "persist_directory = \"./faiss_merged_final\"\n",
    "\n",
    "try:\n",
    "    faiss_db = FAISS.load_local(\n",
    "        folder_path=persist_directory,\n",
    "        embeddings=embedding_model,\n",
    "        allow_dangerous_deserialization=True  # 신뢰할 수 있는 소스에서만 사용\n",
    "    )\n",
    "    \n",
    "    # embedding_function 수정\n",
    "    faiss_db.embedding_function = lambda text: (\n",
    "        embedding_model.encode(text) if isinstance(text, str) else embedding_model.encode(str(text))\n",
    "    )\n",
    "    print('faiss_db 로드 성공')\n",
    "except Exception as e:\n",
    "    print(f\"📌 FAISS.load_local() 반환값: {type(faiss_db)}\")\n",
    "    print(f\"FAISS 데이터베이스 로드 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4607b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4bit 양자화 활성화\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 계산 타입 설정 (float16이 일반적)\n",
    "    bnb_4bit_use_double_quant=True,  # 더블 양자화 사용 (메모리 절약)\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 (NF4) 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0bffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit 양자화 활성화\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # 계산 정밀도 설정\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NF4 양자화 방식 사용 (효율적)\n",
    "    bnb_4bit_use_double_quant=True,  # 이중 양자화 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40645602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/DeepSeek-R1-Distill-Qwen-14B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ 올바른 양자화 설정 적용\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ 자동 GPU 배치\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/transformers/modeling_utils.py:4014\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4005\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4007\u001b[0m     (\n\u001b[1;32m   4008\u001b[0m         model,\n\u001b[1;32m   4009\u001b[0m         missing_keys,\n\u001b[1;32m   4010\u001b[0m         unexpected_keys,\n\u001b[1;32m   4011\u001b[0m         mismatched_keys,\n\u001b[1;32m   4012\u001b[0m         offload_index,\n\u001b[1;32m   4013\u001b[0m         error_msgs,\n\u001b[0;32m-> 4014\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4034\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/transformers/modeling_utils.py:4502\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4498\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4499\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4500\u001b[0m                 )\n\u001b[1;32m   4501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4502\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4505\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4506\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4508\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4509\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4516\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4517\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4518\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/transformers/modeling_utils.py:975\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    973\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:238\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    235\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    237\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 238\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:332\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_quantized:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:297\u001b[0m, in \u001b[0;36mParams4bit._quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[1;32m    296\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 297\u001b[0m     w_4bit, quant_state \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_4bit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompress_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/bitsandbytes/functional.py:1263\u001b[0m, in \u001b[0;36mquantize_4bit\u001b[0;34m(A, absmax, out, blocksize, compress_statistics, quant_type, quant_storage)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlockwise quantization only supports 16/32-bit floats, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1261\u001b[0m post_call(A\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1263\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[43mget_4bit_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compress_statistics:\n\u001b[1;32m   1266\u001b[0m     offset \u001b[38;5;241m=\u001b[39m absmax\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/bitsandbytes/functional.py:1117\u001b[0m, in \u001b[0;36mget_4bit_type\u001b[0;34m(typename, device, blocksize)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypename \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m data\u001b[38;5;241m.\u001b[39mdiv_(data\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# 모델과 토크나이저 로드 (CUDA 사용)\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,  # ✅ 올바른 양자화 설정 적용\n",
    "    device_map=\"auto\",  # ✅ 자동 GPU 배치\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6446fa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 파이프라인 생성\u001b[39;00m\n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 6\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m      7\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      8\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,  \u001b[38;5;66;03m# 생성할 최대 토큰 수 증가\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,        \u001b[38;5;66;03m# 샘플링 활성화\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,      \n\u001b[1;32m     11\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,             \n\u001b[1;32m     12\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.05\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# LangChain의 HuggingFacePipeline 사용\u001b[39;00m\n\u001b[1;32m     15\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mpipe)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 파이프라인 생성\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,  # 생성할 최대 토큰 수 증가\n",
    "    do_sample=True,        # 샘플링 활성화\n",
    "    temperature=0.1,      \n",
    "    top_k=50,             \n",
    "    repetition_penalty=1.05\n",
    ")\n",
    "# LangChain의 HuggingFacePipeline 사용\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5890507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "deepseek_template = \"\"\"\n",
    "<|system|>\n",
    "You are a friendly chatbot specializing in artworks and general conversations.\n",
    "Your primary role is to answer questions **accurately based on the provided document (context)**. \n",
    "If the requested information is not found in the document, respond with:\n",
    "\"문서에 해당 정보가 없습니다.\" \n",
    "\n",
    "However, if the question is a general conversation or does not relate to the document, you should respond naturally as a conversational chatbot. \n",
    "You can talk about art history, artists, exhibitions, and general topics such as daily life, technology, and culture. \n",
    "Maintain a friendly and engaging tone, ensuring all responses are written in Korean.\n",
    "Use **beautiful Markdown formatting** (headings, bullet points, **bold** or *italic* text) to enhance readability.\n",
    "You must include the artwork number in your response.\n",
    "\n",
    "<|context|>\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "Question: {question}\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exaone_template = '''\n",
    "<|system|>\n",
    "You are an AI assistant tasked with refining and polishing the provided logical reasoning into a final answer in Korean.  \n",
    "Your role is to produce a clear, concise, and well-structured response that maintains the original meaning and key details.  \n",
    "Ensure that your final answer is written in Korean and uses **beautiful Markdown formatting** (e.g., headings, bullet points, **bold** or *italic* text) to enhance readability.  \n",
    "Focus solely on refining the content without adding any new information.\n",
    "You must include the artwork number in your response.\n",
    "\n",
    "<|reasoning|>\n",
    "{reasoning}\n",
    "\n",
    "<|user|>\n",
    "Based on the above reasoning, please generate a refined and final answer in Korean.\n",
    "\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# DeepSeek 템플릿 생성\n",
    "deepseek_prompt = ChatPromptTemplate.from_template(deepseek_template)\n",
    "\n",
    "# EXAONE 템플릿 생성\n",
    "exaone_prompt = ChatPromptTemplate.from_template(exaone_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "747bc299-6dde-4a89-beeb-0f330ac28baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = '''\n",
    "<|system|>\n",
    "You are a friendly chatbot specializing in artworks and general conversations.\n",
    "Your primary role is to answer questions strictly based on the information provided in the document (context). \n",
    "If the requested information is not found in the document, respond with:\n",
    "\"The document does not contain this information.\" \n",
    "\n",
    "However, if the question is a general conversation or does not relate to the document, you should respond naturally as a conversational chatbot. \n",
    "You can talk about art history, artists, exhibitions, and general topics such as daily life, technology, and culture. \n",
    "Maintain a friendly and engaging tone, ensuring all responses are written in Korean.\n",
    "Use **beautiful Markdown formatting** (headings, bullet points, bold or italic text) to enhance readability.\n",
    "You must include artwork number.\n",
    "\n",
    "<|context|>\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "Question: {question}\n",
    "\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "137611cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_db.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 5,                # 검색 결과 개수\n",
    "        \"fetch_k\": 15,         # 더 많은 결과 가져오기\n",
    "        \"mmr\": True,           # MMR 활성화\n",
    "        \"mmr_beta\": 0.3      # 다양성과 관련성 간 균형\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e8023be-86e0-487d-b773-df209eac7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MarkdownOutputParser:\n",
    "    \"\"\"Enhanced Markdown parser with additional formatting options.\"\"\"\n",
    "\n",
    "    def __call__(self, llm_output):\n",
    "        \"\"\"Extracts the assistant's response from after the </think> tag and formats it in Markdown.\"\"\"\n",
    "        if not llm_output or llm_output.strip() == \"\":\n",
    "            return \"❌ 모델에서 응답을 생성하지 못했습니다.\"\n",
    "\n",
    "        # \"</think>\" 이후 텍스트 추출\n",
    "        match = re.search(r\"</think>\\s*(.*)\", llm_output, re.DOTALL)\n",
    "        extracted_text = match.group(1).strip() if match else llm_output.strip()\n",
    "\n",
    "        # Markdown 형식 적용\n",
    "        formatted_output = f\"\"\"\n",
    "### **🔹 모델 응답 결과**\n",
    "\n",
    "{extracted_text}\n",
    "\"\"\"\n",
    "        return formatted_output.strip()  # 양 끝 공백 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f0b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MarkdownOutputParser2:\n",
    "    \"\"\"Enhanced Markdown parser with additional formatting options.\"\"\"\n",
    "\n",
    "    def __call__(self, llm_output):\n",
    "        \"\"\"Extracts the assistant's response from after the </think> tag and formats it in Markdown.\"\"\"\n",
    "        if not llm_output or llm_output.strip() == \"\":\n",
    "            return \"❌ 모델에서 응답을 생성하지 못했습니다.\"\n",
    "\n",
    "        # \"</think>\" 이후 텍스트 추출\n",
    "        match = re.search(r\"<\\|assistant\\|>\\s*(.*)\", llm_output, re.DOTALL)\n",
    "        extracted_text = match.group(1).strip() if match else llm_output.strip()\n",
    "\n",
    "        # Markdown 형식 적용\n",
    "        formatted_output = f\"\"\"\n",
    "### **🔹 모델 응답 결과**\n",
    "\n",
    "{extracted_text}\n",
    "\"\"\"\n",
    "        return formatted_output.strip()  # 양 끝 공백 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4072d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# 🔹 EXAONE 모델 로드\n",
    "exaone_model_id = \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\"\n",
    "exaone_tokenizer = AutoTokenizer.from_pretrained(exaone_model_id)\n",
    "exaone_model = AutoModelForCausalLM.from_pretrained(\n",
    "    exaone_model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda\",  # CUDA에서 자동 배치\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b1bd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43857/1623051539.py:15: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  exaone_llm = HuggingFacePipeline(pipeline=exaone_pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 파이프라인 생성\n",
    "exaone_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=exaone_model,\n",
    "    tokenizer=exaone_tokenizer,\n",
    "    max_new_tokens=1024,  # 생성할 최대 토큰 수 증가\n",
    "    do_sample=True,        # 샘플링 활성화\n",
    "    temperature=0.2,      \n",
    "    top_k=50,             \n",
    "    repetition_penalty=1.05\n",
    ")\n",
    "# LangChain의 HuggingFacePipeline 사용\n",
    "exaone_llm = HuggingFacePipeline(pipeline=exaone_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "123efb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableMap\n",
    "\n",
    "chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": retriever,               # Retriever에서 반환된 값을 가져옴\n",
    "        \"question\": RunnablePassthrough()   # 질문은 그대로 전달\n",
    "    })\n",
    "    | (lambda x: {\n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in x[\"context\"]]),\n",
    "        \"question\": x[\"question\"]\n",
    "    })  # context를 문자열로 변환\n",
    "    | prompt                               # Prompt Template에 전달\n",
    "    | exaone_llm                                  # LLM으로 응답 생성\n",
    "    | MarkdownOutputParser2()                    # 응답을 문자열로 변환\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5af389",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableLambda\n\u001b[1;32m      3\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     retriever\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m|\u001b[39m RunnableLambda(\u001b[38;5;28;01mlambda\u001b[39;00m docs: {  \n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]),  \n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query \n\u001b[1;32m      8\u001b[0m     })\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m|\u001b[39m deepseek_prompt\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;241m|\u001b[39m \u001b[43mllm\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m|\u001b[39m MarkdownOutputParser()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m|\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: x})\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m|\u001b[39m exaone_prompt\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m|\u001b[39m exaone_llm\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m|\u001b[39m MarkdownOutputParser2()\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    retriever\n",
    "    | RunnableLambda(lambda docs: {  \n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in docs]),  \n",
    "        \"question\": query \n",
    "    })\n",
    "    | deepseek_prompt\n",
    "    | llm\n",
    "    | MarkdownOutputParser()\n",
    "    | (lambda x: {\"reasoning\": x})\n",
    "    | exaone_prompt\n",
    "    | exaone_llm\n",
    "    | MarkdownOutputParser2()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e572399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss_db 로드 성공\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# 기존 DB 로드 \n",
    "persist_directory = \"./faiss_artworks_0324_pdf2\"\n",
    "\n",
    "try:\n",
    "    faiss_db = FAISS.load_local(\n",
    "        folder_path=persist_directory,\n",
    "        embeddings=embedding_model,\n",
    "        allow_dangerous_deserialization=True  # 신뢰할 수 있는 소스에서만 사용\n",
    "    )\n",
    "    \n",
    "    # embedding_function 수정\n",
    "    faiss_db.embedding_function = lambda text: (\n",
    "        embedding_model.encode(text) if isinstance(text, str) else embedding_model.encode(str(text))\n",
    "    )\n",
    "    print('faiss_db 로드 성공')\n",
    "except Exception as e:\n",
    "    print(f\"📌 FAISS.load_local() 반환값: {type(faiss_db)}\")\n",
    "    print(f\"FAISS 데이터베이스 로드 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "069c15b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 index 벡터 수: 746\n",
      "🧩 docstore 문서 수: 746\n",
      "🧩 index_to_docstore_id: 746\n"
     ]
    }
   ],
   "source": [
    "print(\"🧩 index 벡터 수:\", faiss_db.index.ntotal)\n",
    "print(\"🧩 docstore 문서 수:\", len(faiss_db.docstore._dict))\n",
    "print(\"🧩 index_to_docstore_id:\", len(faiss_db.index_to_docstore_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19e114ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(list(faiss_db.index_to_docstore_id.values())[0]))  # 예: <class 'str'> 나 <class 'int'>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb2b4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"미술품 수집의 성과는?\" # 746, 12403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ef549de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "12593",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/runnables/base.py:3727\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3726\u001b[0m         ]\n\u001b[0;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/runnables/base.py:3711\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[0;34m(step, input, config, key)\u001b[0m\n\u001b[1;32m   3709\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   3710\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m-> 3711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3713\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/retrievers.py:266\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    265\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    269\u001b[0m         result,\n\u001b[1;32m    270\u001b[0m     )\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/retrievers.py:259\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 259\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:1080\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m   1078\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1080\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1082\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m   1084\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[1;32m   1085\u001b[0m             )\n\u001b[1;32m   1086\u001b[0m         )\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:641\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    623\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    628\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    629\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:514\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[0;32m--> 514\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score_by_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:425\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[0;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# This happens when not enough docs are returned.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m _id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_to_docstore_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    426\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39msearch(_id)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, Document):\n",
      "\u001b[0;31mKeyError\u001b[0m: 12593"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "501b6c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Content: {\n",
      "    \"title\": \"소리 없는 소리\",\n",
      "    \"title_eng\": \"The Silent Sound \",\n",
      "    \"artist\": \"김은설\",\n",
      "    \"artist_eng\": \"KIM Eunseol\",\n",
      "    \"artwork_number\": 11301,\n",
      "    \"year\": \"N/A\",\n",
      "    \"size\": \"5분 5초\",\n",
      "    \"materials\": \"3채널 영상, 컬러, 사운드\",\n",
      "    \"category\": \"뉴미디어\",\n",
      "    \"description\": \"김은설(金珢雪, 1988- )은 청주대학교에서 회화를 전공한 후 회화, 드로잉, 영상 설치, 퍼포먼스 등 다양한 매체를 다루며 작업하고 있다. 작가는 주로 소리를 촉각이나 시각으로 표현하는 등 청각을 또 다른 감각으로 변환하여 다룸으로써 신체 감각과 소통에 대한 본질적인 질문을 던지는 작업에 집중하고 있다. 2024년 《여기 닿은 노래》(아르코미술관), 2022년 《듣다 보다》(JCC아트센터) 등 다수 전시에 참여한 바 있으며, 2023년 <체록(體錄)-다른 존재의 몸짓을 내 몸으로 기록하기>(청년예술청 SAPY 그레이룸) 공연, 2021년부터 진행한 <므브프> 프로젝트 등 다양한 유형의 활동을 지속하고 있다.<소리 없는 소리>는 작가가 일상에서 흥미로운 소리가 날 것 같은 장면들을 촬영한 후, 해당 장면에서 느껴지는 촉감이나 예상되는 소리를 수집하여 영상에 덧입힌 작품이다. 작가는 항상 보청기를 통해 교정된 소리를 듣기 때문에 본인이 감각하는 청각이 불완전하다는 의식으로 일상에서는 청각에만 의존하지 않고 눈으로 본 형상을 통해 소리를 본다고 한다. 작품은 작가 스스로 ‘감각하는 소리’와 ‘보는 소리’ 사이의 일치화 과정을 담고 있으며, 영상과 사운드의 일치 혹은 불일치를 통해 관객에게 듣는다는 것이 무엇인지, 나아가 감각한다는 것의 본질에 대해 재고하게 한다.\",\n",
      "    \"read_count\": 66,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "Metadata: {'title': '소리 없는 소리', 'artist': '김은설', 'year': 'N/A', 'read_count': 66, 'collection': '국립현대미술관'}\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "Content: {\n",
      "    \"title\": \"소리-3\",\n",
      "    \"title_eng\": \"Sound-3\",\n",
      "    \"artist\": \"권정호\",\n",
      "    \"artist_eng\": \"KWON Jungho\",\n",
      "    \"artwork_number\": 2543,\n",
      "    \"year\": \"1985\",\n",
      "    \"size\": \"198×350(d 3.6)\",\n",
      "    \"materials\": \"캔버스에 유화 물감\",\n",
      "    \"category\": \"회화 II\",\n",
      "    \"description\": \"학산(鶴山) 권정호(1944- )의 <사운드 III>(1985)는 일종의 사회적 고발을 위한 방법으로 해골 대신 스피커를 선택하였다. 과학기술의 이기(利器)가 궁극적으로는 인간의 사고와 감정을 지배하고 환경을 조건 지우는 억압의 근원이라고 간파한 것이다. 반복되는 해악한 전자음의 파장은 강렬한 파장의 색점으로 공간 속에 무한히 확산되고 있다. 작가가 추구하는 목표는 음과 양의 조화로운 세계이며 이를 위해 불교미술의 단순하고 원색적인 요소들이 적용되었다.\",\n",
      "    \"read_count\": 29,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "Metadata: {'title': '소리-3', 'artist': '권정호', 'year': '1985', 'read_count': 29, 'collection': '국립현대미술관'}\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "Content: {\n",
      "    \"title\": \"세상은 이렇게 종말을 맞이한다 쿵 소리 한 번 없이 흐느낌으로\",\n",
      "    \"title_eng\": \"This is the Way the World Ends Not with a Bang but a Whimper\",\n",
      "    \"artist\": \"송상희\",\n",
      "    \"artist_eng\": \"SONG Sanghee\",\n",
      "    \"artwork_number\": 8007,\n",
      "    \"year\": \"2017\",\n",
      "    \"size\": \"720×500(가변적)\",\n",
      "    \"materials\": \"타일벽,8채널 음향 설치\",\n",
      "    \"category\": \"뉴미디어\",\n",
      "    \"description\": \"송상희(宋相憙, 1970-)는 현재 서울과 암스테르담을 중심으로 활동하고 있으며, 영상, 드로잉, 사진, 퍼포먼스 등의 다양한 매체를 통해 신화나 사회의 관습, 일상을 재맥락화하고 정치･사회･문화적 쟁점을 압축적으로 표현하는 작업을 진행하고 있다. 2000년대의 작업은 근대성을 경험한 여성의 시선으로 비극적 장면과 구조화된 신체를 재현하며 사회 속 여성의 위치에 대한 질문을 던지는 것이었다. 2010년 이후의 작업에서는 더욱 섬세하고 다층적으로 수집·연구된 역사적 사료를 기반으로 역사의 현장에서 잊힌 것들, 그 순간 속에 머문 찬란한 것들과의 관계 맺기를 이어 간다.<세상은 이렇게 종말을 맞이한다 쿵 소리 한 번 없이 흐느낌으로>는 작가가 여러 시대에 걸친 여러 지역의 폭발 장면을 드로잉하여 델프트 블루(delft blue) 타일로 제작한 벽이다. 제2차 세계대전의 원자 폭탄부터 ISIS의 공습까지 다양한 폭발 이미지를 담은 타일을 드로잉 원본의 정렬 순서에 따르지 않고 무작위로 배치했다. 화면은 마치 먼지나 구름을 묘사한 파랑색 모노크롬 회화처럼 보이지만 실제로는 수많은 사람들이 죽고 도시가 파괴된 장면들을 모은 것이다. 또한 타일 벽에 설치된 스피커에서는 55개국의 언어로 녹음된 인사말이 흘러나오는데, 이는 1977년에 우주로 발사된 보이저 탐사선에 실렸던 육성 파일이다. 픽셀들의 집합체가 되어 버린 비극적인 이미지 앞에서 우주로 보내는 안부 인사를 듣는 것은 전쟁이라는 파국의 현실이 우리 옆에 존재하지만 이미 이미지화된 현실에 익숙해진 삶과 닮아 있다. 작품의 제목은 T. S. 엘리엇(1888-1965)의 시 「텅 빈 사람들(The Hollow man)」(1925)의 마지막 구절에서 따온 것이다. 시의 마지막 구절은 흐느끼는 울음소리가 쿵 소리와 같은 크고 뚜렷한 소리보다 더 큰 고독과 절망감을 안겨 준다는 메시지를 암시하고 있다. 시를 활용한 작품의 제목은 폭발음이 없는 폭발 이미지가 전달하는 공허함을 증폭한다.\",\n",
      "    \"read_count\": 87,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "Metadata: {'title': '세상은 이렇게 종말을 맞이한다 쿵 소리 한 번 없이 흐느낌으로', 'artist': '송상희', 'year': '2017', 'read_count': 87, 'collection': '국립현대미술관'}\n",
      "--------------------------------------------------\n",
      "Document 4:\n",
      "Content: {\n",
      "    \"title\": \"소리 I\",\n",
      "    \"title_eng\": \"Sound I\",\n",
      "    \"artist\": \"오세열\",\n",
      "    \"artist_eng\": \"OH Seyeol\",\n",
      "    \"artwork_number\": 2706,\n",
      "    \"year\": \"1987\",\n",
      "    \"size\": \"150×100.2×(3)\",\n",
      "    \"materials\": \"캔버스에 아크릴릭 물감, 구아슈, 유화 물감, 넥타이, 콜라주\",\n",
      "    \"category\": \"회화 II\",\n",
      "    \"description\": \"오세열(1945- )의 <소리 I>(1987)은 커다란 칠판 같은 캔버스에 분필을 연상시키는 흰색의 선들을 표현하였다. 화면에는 미궁처럼 복잡하게 얽힌 선들 속에 간간이 도형과 숫자, 알파벳 등 서로 아무 관계없는 무의미한 기호들이 나열되어있다. 마치 아이들이 교실 칠판에 한 낙서 같으며 화면에 그려진 넥타이는 선생님을 대표하는 이미지와 연결된다. 한편 좌우의 노란색과 청색 넥타이는 손으로 그린 이미지이고 반면에 검은 줄무늬 넥타이는 실제의 것을 그대로 부착하여 역설적인 효과를 시도하였다.\",\n",
      "    \"read_count\": 22,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "Metadata: {'title': '소리 I', 'artist': '오세열', 'year': '1987', 'read_count': 22, 'collection': '국립현대미술관'}\n",
      "--------------------------------------------------\n",
      "Document 5:\n",
      "Content: {\n",
      "    \"title\": \"세월의 소리\",\n",
      "    \"title_eng\": \"Sound of Time\",\n",
      "    \"artist\": \"제정자\",\n",
      "    \"artist_eng\": \"JE Jungja\",\n",
      "    \"artwork_number\": 2722,\n",
      "    \"year\": \"1987\",\n",
      "    \"size\": \"145×112×(3)\",\n",
      "    \"materials\": \"캔버스에 유화 물감\",\n",
      "    \"category\": \"회화 II\",\n",
      "    \"description\": \"제정자(1937- )는 <세월의 소리>(1987)에서 청백색 모노크롬(Monochrome)의 공간 속에 '기운생동(氣運生動)'의 느낌을 전해주는 수평적인 짧은 획들을 전개시킨다. 특정한 이미지를 떠나 철저히 추상화된 이 작품은 필선의 농담 강·약에 의한 유동적인 획선을 통해 평면화되었다. 화면 속에 드문드문 드러나는 직선의 색띠와 색면은 율동적인 필선에 가려 부분적으로 지워지기도 하면서 유동적인 곡선 형상과 공존관계를 유지하며 화면의 긴장감을 조성한다.\",\n",
      "    \"read_count\": 25,\n",
      "    \"collection\": \"국립현대미술관\"\n",
      "}\n",
      "Metadata: {'title': '세월의 소리', 'artist': '제정자', 'year': '1987', 'read_count': 25, 'collection': '국립현대미술관'}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1049/3695306690.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content}\")  # 문서의 실제 내용\n",
    "    print(f\"Metadata: {doc.metadata}\")    # 메타데이터 (예: 출처, 페이지 등)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b13d46-a624-421e-80bd-f1f041d77b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 수행: 유사도 점수와 함께 반환\n",
    "docs_and_scores = retriever.vectorstore.similarity_search_with_score(query, k=5)\n",
    "\n",
    "# 검색된 문서 수 출력\n",
    "print(f\"검색된 문서 수: {len(docs_and_scores)}\")\n",
    "\n",
    "# 각 문서의 파일명, 전체 내용, 유사도 점수 출력\n",
    "for i, (doc, score) in enumerate(docs_and_scores, 1):\n",
    "    print(f\"\\n문서 {i}:\")\n",
    "    print(f\"  파일명: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"  유사도 점수: {score:.4f}\")\n",
    "    print(f\"  전체 내용: {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a46bc8",
   "metadata": {},
   "source": [
    "### PDF DB에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b92e93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "docstore1가 dict가 아닙니다: <class 'langchain_community.docstore.in_memory.InMemoryDocstore'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m db2_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./faiss_artworks_0318\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 두 번째 FAISS DB 디렉토리\u001b[39;00m\n\u001b[1;32m     65\u001b[0m merged_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./faiss_merged_db\u001b[39m\u001b[38;5;124m\"\u001b[39m    \u001b[38;5;66;03m# 병합된 결과를 저장할 디렉토리\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mmerge_faiss_databases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb1_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb2_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 47\u001b[0m, in \u001b[0;36mmerge_faiss_databases\u001b[0;34m(db1_dir, db2_dir, merged_dir)\u001b[0m\n\u001b[1;32m     44\u001b[0m     merged_mapping[i \u001b[38;5;241m+\u001b[39m offset] \u001b[38;5;241m=\u001b[39m mapping2[i]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 문서 저장소 병합\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(docstore1, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore1가 dict가 아닙니다: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(docstore1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m merged_docstore\u001b[38;5;241m.\u001b[39mupdate(docstore1)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(docstore2, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore2가 dict가 아닙니다: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(docstore2)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: docstore1가 dict가 아닙니다: <class 'langchain_community.docstore.in_memory.InMemoryDocstore'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def load_faiss_index(db_path):\n",
    "    \"\"\"FAISS 인덱스 로드\"\"\"\n",
    "    index = faiss.read_index(os.path.join(db_path, \"index.faiss\"))\n",
    "    with open(os.path.join(db_path, \"index.pkl\"), \"rb\") as f:\n",
    "        docstore = pickle.load(f)\n",
    "    \n",
    "    # docstore가 튜플이면 첫 번째 요소만 사용\n",
    "    if isinstance(docstore, tuple):\n",
    "        docstore = docstore[0]\n",
    "    \n",
    "    # mapping을 index.pkl에서 로드 (index_to_docstore_id.pkl이 없음)\n",
    "    mapping = {i: str(i) for i in range(index.ntotal)}\n",
    "    \n",
    "    return index, docstore, mapping\n",
    "\n",
    "def merge_faiss_databases(db1_dir, db2_dir, merged_dir):\n",
    "    \"\"\"두 개의 FAISS 데이터베이스 병합\"\"\"\n",
    "    index1, docstore1, mapping1 = load_faiss_index(db1_dir)\n",
    "    index2, docstore2, mapping2 = load_faiss_index(db2_dir)\n",
    "\n",
    "    dim = index1.d  # 첫 번째 인덱스의 차원을 자동 감지\n",
    "    merged_index = faiss.IndexFlatL2(dim)\n",
    "    merged_docstore = {}\n",
    "    merged_mapping = {}\n",
    "\n",
    "    # 첫 번째 DB 추가\n",
    "    num_vectors1 = index1.ntotal\n",
    "    for i in range(num_vectors1):\n",
    "        vec = np.array(index1.reconstruct(i)).reshape(1, dim).astype(\"float32\")\n",
    "        merged_index.add(vec)\n",
    "        merged_mapping[i] = mapping1[i]\n",
    "\n",
    "    # 두 번째 DB 추가 (ID 오프셋 적용)\n",
    "    num_vectors2 = index2.ntotal\n",
    "    offset = num_vectors1\n",
    "    for i in range(num_vectors2):\n",
    "        vec = np.array(index2.reconstruct(i)).reshape(1, dim).astype(\"float32\")\n",
    "        merged_index.add(vec)\n",
    "        merged_mapping[i + offset] = mapping2[i]\n",
    "\n",
    "    # 문서 저장소 병합\n",
    "    assert isinstance(docstore1, dict), f\"docstore1가 dict가 아닙니다: {type(docstore1)}\"\n",
    "    merged_docstore.update(docstore1)\n",
    "    assert isinstance(docstore2, dict), f\"docstore2가 dict가 아닙니다: {type(docstore2)}\"\n",
    "    merged_docstore.update(docstore2)\n",
    "\n",
    "    # 결과 저장\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    faiss.write_index(merged_index, os.path.join(merged_dir, \"index.faiss\"))\n",
    "    with open(os.path.join(merged_dir, \"index.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(merged_docstore, f)\n",
    "    with open(os.path.join(merged_dir, \"index_to_docstore_id.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(merged_mapping, f)\n",
    "    print(f\"병합된 FAISS 데이터베이스 저장 완료: {merged_dir}\")\n",
    "\n",
    "# 예제 실행\n",
    "if __name__ == \"__main__\":\n",
    "    db1_dir = \"./faiss_artworks_0304\"  # 첫 번째 FAISS DB 디렉토리\n",
    "    db2_dir = \"./faiss_artworks_0318\"  # 두 번째 FAISS DB 디렉토리\n",
    "    merged_dir = \"./faiss_merged_db\"    # 병합된 결과를 저장할 디렉토리\n",
    "\n",
    "    merge_faiss_databases(db1_dir, db2_dir, merged_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9d766f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "피클 딕셔너리에 'docstore' 또는 'index_to_docstore_id' 키가 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 저장된 FAISS DB 로드 (각 디렉토리에 저장된 index.faiss, index.pkl 파일 사용)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m db1 \u001b[38;5;241m=\u001b[39m load_local_custom(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_artworks_0304\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings, index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m db2 \u001b[38;5;241m=\u001b[39m \u001b[43mload_local_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfaiss_artworks_0318\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# db2의 벡터들을 db1에 병합 (merge_from 메서드 사용)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m db1\u001b[38;5;241m.\u001b[39mmerge_from(db2)\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mload_local_custom\u001b[0;34m(folder_path, embeddings, index_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m         index_to_docstore_id \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m피클 딕셔너리에 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocstore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 또는 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 키가 없습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m피클 파일의 형식이 예상과 다릅니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: 피클 딕셔너리에 'docstore' 또는 'index_to_docstore_id' 키가 없습니다."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "def load_local_custom(folder_path, embeddings, index_name=\"index\"):\n",
    "    \"\"\"\n",
    "    저장된 FAISS DB 디렉토리에서 index.faiss와 index.pkl 파일을 로드합니다.\n",
    "    pickle 파일이 튜플이면 처음 2개의 값을, 딕셔너리이면 \n",
    "    \"docstore\"와 \"index_to_docstore_id\" 키의 값을 사용합니다.\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    index = faiss.read_index(str(folder / \"index.faiss\"))\n",
    "    pkl_path = folder / f\"{index_name}.pkl\"\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    # data가 튜플인 경우\n",
    "    if isinstance(data, tuple):\n",
    "        if len(data) >= 2:\n",
    "            docstore, index_to_docstore_id = data[0], data[1]\n",
    "        else:\n",
    "            raise ValueError(\"피클 튜플의 길이가 2보다 작습니다.\")\n",
    "    # data가 딕셔너리인 경우 (예: {\"docstore\": ..., \"index_to_docstore_id\": ...})\n",
    "    elif isinstance(data, dict):\n",
    "        if \"docstore\" in data and \"index_to_docstore_id\" in data:\n",
    "            docstore = data[\"docstore\"]\n",
    "            index_to_docstore_id = data[\"index_to_docstore_id\"]\n",
    "        else:\n",
    "            raise ValueError(\"피클 딕셔너리에 'docstore' 또는 'index_to_docstore_id' 키가 없습니다.\")\n",
    "    else:\n",
    "        raise ValueError(\"피클 파일의 형식이 예상과 다릅니다.\")\n",
    "    \n",
    "    return FAISS(embeddings, index, docstore, index_to_docstore_id)\n",
    "\n",
    "# OpenAIEmbeddings 인스턴스 생성 (API 키 등 설정 필요)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 저장된 FAISS DB 로드 (각 디렉토리에 저장된 index.faiss, index.pkl 파일 사용)\n",
    "db1 = load_local_custom(\"faiss_artworks_0304\", embeddings, index_name=\"index\")\n",
    "db2 = load_local_custom(\"faiss_artworks_0318\", embeddings, index_name=\"index\")\n",
    "\n",
    "# db2의 벡터들을 db1에 병합 (merge_from 메서드 사용)\n",
    "db1.merge_from(db2)\n",
    "\n",
    "# 병합된 FAISS DB를 새 디렉토리에 저장\n",
    "db1.save_local(\"faiss_merged_db\")\n",
    "\n",
    "print(\"병합 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2317b04",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InMemoryDocstore' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m문서 ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())[\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m첫 번째 문서 내용: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocstore[\u001b[38;5;28mlist\u001b[39m(docstore\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m10\u001b[39m]]\u001b[38;5;241m.\u001b[39mpage_content[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InMemoryDocstore' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "print(f\"문서 ID: {list(docstore.keys())[10]}\")\n",
    "print(f\"첫 번째 문서 내용: {docstore[list(docstore.keys())[10]].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04a0c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"오지호 풍경 하부 도상의 제작연도는 어떻게 추정해?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fe29183",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find document for id 498, got ID 498 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response) \n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/retrievers.py:266\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    265\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[1;32m    269\u001b[0m         result,\n\u001b[1;32m    270\u001b[0m     )\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/retrievers.py:259\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 259\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:1080\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m   1078\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1080\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1082\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[1;32m   1084\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[1;32m   1085\u001b[0m             )\n\u001b[1;32m   1086\u001b[0m         )\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:641\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    623\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    628\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    629\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:514\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(query)\n\u001b[0;32m--> 514\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score_by_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/env_0217/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:428\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score_by_vector\u001b[0;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39msearch(_id)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, Document):\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find document for id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filter_func(doc\u001b[38;5;241m.\u001b[39mmetadata):\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find document for id 498, got ID 498 not found."
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": query})\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매핑된 문서 ID 확인\n",
    "for vector_id in range(faiss_index.ntotal):\n",
    "    if vector_id in index_to_docstore_id:\n",
    "        doc_id = index_to_docstore_id[vector_id]\n",
    "        print(f\"벡터 ID: {vector_id}, 문서 ID: {doc_id}, 문서 내용: {docstore[doc_id].page_content[:100]}...\")\n",
    "    else:\n",
    "        print(f\"벡터 ID: {vector_id}에 대한 문서 ID가 매핑되지 않았습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be890d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_0217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
