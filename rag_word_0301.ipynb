{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e06783-f083-424c-9ace-51f3c3bf5e00",
   "metadata": {},
   "source": [
    "### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacafc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt  # 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f425499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # PyTorch 버전\n",
    "print(torch.version.cuda)  # PyTorch가 사용하는 CUDA 버전\n",
    "print(torch.cuda.is_available())  # GPU 사용 가능 여부\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d49d96-9721-43a6-8482-9158d25c93fe",
   "metadata": {},
   "source": [
    "### 2. 문서 split 및 Chroma를 활용한 vector store 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d82f23-0f73-4879-8a69-46d5237a47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chae/env_0217/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로그인 상태입니다. 사용자: chaeeee\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"로그인 상태입니다. 사용자: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(\"로그인되지 않았거나 토큰이 유효하지 않습니다.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ea6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. JSON 파일 경로 설정\n",
    "json_path = \"./json_data.json\"  # 단일 JSON 파일 경로\n",
    "\n",
    "# 2. JSON 데이터 불러오기\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# 3. {}를 기준으로 JSON 데이터 분할 및 Document 객체 생성\n",
    "documents = []\n",
    "\n",
    "for data in tqdm(all_data, desc=\"Generating Documents\", unit=\"entry\", ncols=80):\n",
    "    metadata = {\n",
    "        \"title\": data.get('title', 'N/A'),\n",
    "        \"artist\": data.get('artist', 'N/A'),\n",
    "        \"year\": data.get('year', 'N/A'),\n",
    "        \"read_count\": data.get('read_count', 0)\n",
    "    }\n",
    "\n",
    "    # JSON 데이터의 각 항목을 Document 객체로 변환\n",
    "    doc_content = json.dumps(data, ensure_ascii=False, indent=4)\n",
    "    documents.append(Document(\n",
    "        page_content=doc_content.strip(),\n",
    "        metadata=metadata\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102d292-48e8-4eeb-9280-79dc003ed796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 Document 객체의 텍스트 길이 확인\n",
    "len(documents[11001].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec14a4-2333-4381-8152-dc30d011ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 800자를 초과하는 Document 개수 세기\n",
    "over_800_count = sum(1 for doc in documents if len(doc.page_content) > 800)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"800자를 초과하는 Document 개수: {over_800_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1ef186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.3.1, however, your version is 3.2.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss  # FAISS 라이브러리 필요\n",
    "\n",
    "# 1. 임베딩 초기화\n",
    "embedding_model = SentenceTransformer(\"nlpai-lab/KURE-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce036d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 문서 데이터와 메타데이터 분리\n",
    "texts = [doc.page_content for doc in documents]  # 문서 텍스트\n",
    "metadatas = [doc.metadata for doc in documents]  # 문서 메타데이터\n",
    "\n",
    "# 3. 문서 임베딩 생성\n",
    "embeddings = embedding_model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23144ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FAISS 인덱스 생성\n",
    "embedding_dim = embeddings.shape[1]  # 벡터 차원 확인\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)  # L2 거리 기반 인덱스\n",
    "faiss_index.add(embeddings)  # 벡터 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba322337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# 5. Docstore 생성\n",
    "# 각 문서에 고유 ID를 부여해 InMemoryDocstore 생성\n",
    "docstore = InMemoryDocstore({i : doc for i, doc in enumerate(documents)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd078ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. FAISS 벡터스토어 생성\n",
    "def embed_query(text):\n",
    "    return embedding_model.encode([text])[0]  # 단일 쿼리 텍스트를 임베딩\n",
    "\n",
    "index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "\n",
    "faiss_db = FAISS(\n",
    "    embedding_function=embed_query,\n",
    "    index=faiss_index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")\n",
    "\n",
    "# 7. FAISS 데이터베이스 저장\n",
    "faiss_db.save_local(\"./faiss_artworks_0304_artworks\")\n",
    "print(\"FAISS 데이터베이스가 성공적으로 저장되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 검색 테스트\n",
    "query = \"박승무의 설경\"\n",
    "results = faiss_db.similarity_search(query, k=5)\n",
    "\n",
    "# 10. 검색 결과 출력\n",
    "for result in results:\n",
    "    print(\"문서 텍스트:\", result.page_content)\n",
    "    print(\"문서 메타데이터:\", result.metadata)\n",
    "    print('=------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"nlpai-lab/KURE-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a00995-3f64-49d2-8b0f-033e8b640085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 데이터베이스가 성공적으로 로드되었습니다!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# 기존 DB 로드 \n",
    "persist_directory = \"./faiss_combined\"\n",
    "\n",
    "try:\n",
    "    faiss_db = FAISS.load_local(\n",
    "        folder_path=persist_directory,\n",
    "        embeddings=embedding_model,\n",
    "        allow_dangerous_deserialization=True  # 신뢰할 수 있는 소스에서만 사용\n",
    "    )\n",
    "    \n",
    "    # embedding_function 수정\n",
    "    faiss_db.embedding_function = lambda text: (\n",
    "        embedding_model.encode(text) if isinstance(text, str) else embedding_model.encode(str(text))\n",
    "    )\n",
    "    \n",
    "    print(\"FAISS 데이터베이스가 성공적으로 로드되었습니다!\")\n",
    "except Exception as e:\n",
    "    print(f\"FAISS 데이터베이스 로드 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4607b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4bit 양자화 활성화\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 계산 타입 설정 (float16이 일반적)\n",
    "    bnb_4bit_use_double_quant=True,  # 더블 양자화 사용 (메모리 절약)\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 (NF4) 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0bffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit 양자화 활성화\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # 계산 정밀도 설정\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NF4 양자화 방식 사용 (효율적)\n",
    "    bnb_4bit_use_double_quant=True,  # 이중 양자화 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40645602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "# 모델과 토크나이저 로드 (CUDA 사용)\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,  # ✅ 올바른 양자화 설정 적용\n",
    "    device_map=\"auto\",  # ✅ 자동 GPU 배치\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6446fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 파이프라인 생성\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,  # 생성할 최대 토큰 수 증가\n",
    "    do_sample=True,        # 샘플링 활성화\n",
    "    temperature=0.1,      \n",
    "    top_k=50,             \n",
    "    repetition_penalty=1.05\n",
    ")\n",
    "# LangChain의 HuggingFacePipeline 사용\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "deepseek_template = \"\"\"\n",
    "<|system|>\n",
    "You are a friendly chatbot specializing in artworks and general conversations.\n",
    "Your primary role is to answer questions **accurately based on the provided document (context)**. \n",
    "If the requested information is not found in the document, respond with:\n",
    "\"문서에 해당 정보가 없습니다.\" \n",
    "\n",
    "However, if the question is a general conversation or does not relate to the document, you should respond naturally as a conversational chatbot. \n",
    "You can talk about art history, artists, exhibitions, and general topics such as daily life, technology, and culture. \n",
    "Maintain a friendly and engaging tone, ensuring all responses are written in Korean.\n",
    "Use **beautiful Markdown formatting** (headings, bullet points, **bold** or *italic* text) to enhance readability.\n",
    "You must include the artwork number in your response.\n",
    "\n",
    "<|context|>\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "Question: {question}\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exaone_template = '''\n",
    "<|system|>\n",
    "You are an AI assistant tasked with refining and polishing the provided logical reasoning into a final answer in Korean.  \n",
    "Your role is to produce a clear, concise, and well-structured response that maintains the original meaning and key details.  \n",
    "Ensure that your final answer is written in Korean and uses **beautiful Markdown formatting** (e.g., headings, bullet points, **bold** or *italic* text) to enhance readability.  \n",
    "Focus solely on refining the content without adding any new information.\n",
    "You must include the artwork number in your response.\n",
    "\n",
    "<|reasoning|>\n",
    "{reasoning}\n",
    "\n",
    "<|user|>\n",
    "Based on the above reasoning, please generate a refined and final answer in Korean.\n",
    "\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# DeepSeek 템플릿 생성\n",
    "deepseek_prompt = ChatPromptTemplate.from_template(deepseek_template)\n",
    "\n",
    "# EXAONE 템플릿 생성\n",
    "exaone_prompt = ChatPromptTemplate.from_template(exaone_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "747bc299-6dde-4a89-beeb-0f330ac28baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = '''\n",
    "<|system|>\n",
    "You are a friendly chatbot specializing in artworks and general conversations.\n",
    "Your primary role is to answer questions strictly based on the information provided in the document (context). \n",
    "If the requested information is not found in the document, respond with:\n",
    "\"The document does not contain this information.\" \n",
    "\n",
    "However, if the question is a general conversation or does not relate to the document, you should respond naturally as a conversational chatbot. \n",
    "You can talk about art history, artists, exhibitions, and general topics such as daily life, technology, and culture. \n",
    "Maintain a friendly and engaging tone, ensuring all responses are written in Korean.\n",
    "Use **beautiful Markdown formatting** (headings, bullet points, bold or italic text) to enhance readability.\n",
    "You must include artwork number.\n",
    "\n",
    "<|context|>\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "Question: {question}\n",
    "\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137611cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_db.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 5,                # 검색 결과 개수\n",
    "        \"fetch_k\": 15,         # 더 많은 결과 가져오기\n",
    "        \"mmr\": True,           # MMR 활성화\n",
    "        \"mmr_beta\": 0.3      # 다양성과 관련성 간 균형\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8023be-86e0-487d-b773-df209eac7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MarkdownOutputParser:\n",
    "    \"\"\"Enhanced Markdown parser with additional formatting options.\"\"\"\n",
    "\n",
    "    def __call__(self, llm_output):\n",
    "        \"\"\"Extracts the assistant's response from after the </think> tag and formats it in Markdown.\"\"\"\n",
    "        if not llm_output or llm_output.strip() == \"\":\n",
    "            return \"❌ 모델에서 응답을 생성하지 못했습니다.\"\n",
    "\n",
    "        # \"</think>\" 이후 텍스트 추출\n",
    "        match = re.search(r\"</think>\\s*(.*)\", llm_output, re.DOTALL)\n",
    "        extracted_text = match.group(1).strip() if match else llm_output.strip()\n",
    "\n",
    "        # Markdown 형식 적용\n",
    "        formatted_output = f\"\"\"\n",
    "### **🔹 모델 응답 결과**\n",
    "\n",
    "{extracted_text}\n",
    "\"\"\"\n",
    "        return formatted_output.strip()  # 양 끝 공백 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84f0b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MarkdownOutputParser2:\n",
    "    \"\"\"Enhanced Markdown parser with additional formatting options.\"\"\"\n",
    "\n",
    "    def __call__(self, llm_output):\n",
    "        \"\"\"Extracts the assistant's response from after the </think> tag and formats it in Markdown.\"\"\"\n",
    "        if not llm_output or llm_output.strip() == \"\":\n",
    "            return \"❌ 모델에서 응답을 생성하지 못했습니다.\"\n",
    "\n",
    "        # \"</think>\" 이후 텍스트 추출\n",
    "        match = re.search(r\"<\\|assistant\\|>\\s*(.*)\", llm_output, re.DOTALL)\n",
    "        extracted_text = match.group(1).strip() if match else llm_output.strip()\n",
    "\n",
    "        # Markdown 형식 적용\n",
    "        formatted_output = f\"\"\"\n",
    "### **🔹 모델 응답 결과**\n",
    "\n",
    "{extracted_text}\n",
    "\"\"\"\n",
    "        return formatted_output.strip()  # 양 끝 공백 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4072d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:23<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# 🔹 EXAONE 모델 로드\n",
    "exaone_model_id = \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\"\n",
    "exaone_tokenizer = AutoTokenizer.from_pretrained(exaone_model_id)\n",
    "exaone_model = AutoModelForCausalLM.from_pretrained(\n",
    "    exaone_model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda\",  # CUDA에서 자동 배치\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b1bd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1464/2480121385.py:15: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  exaone_llm = HuggingFacePipeline(pipeline=exaone_pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 파이프라인 생성\n",
    "exaone_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=exaone_model,\n",
    "    tokenizer=exaone_tokenizer,\n",
    "    max_new_tokens=1024,  # 생성할 최대 토큰 수 증가\n",
    "    do_sample=True,        # 샘플링 활성화\n",
    "    temperature=0.1,      \n",
    "    top_k=50,             \n",
    "    repetition_penalty=1.05\n",
    ")\n",
    "# LangChain의 HuggingFacePipeline 사용\n",
    "exaone_llm = HuggingFacePipeline(pipeline=exaone_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "123efb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableMap\n",
    "\n",
    "chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": retriever,               # Retriever에서 반환된 값을 가져옴\n",
    "        \"question\": RunnablePassthrough()   # 질문은 그대로 전달\n",
    "    })\n",
    "    | (lambda x: {\n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in x[\"context\"]]),\n",
    "        \"question\": x[\"question\"]\n",
    "    })  # context를 문자열로 변환\n",
    "    | prompt                               # Prompt Template에 전달\n",
    "    | exaone_llm                                  # LLM으로 응답 생성\n",
    "    | MarkdownOutputParser2()                    # 응답을 문자열로 변환\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5af389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "chain = (\n",
    "    retriever\n",
    "    | RunnableLambda(lambda docs: {  \n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in docs]),  \n",
    "        \"question\": query \n",
    "    })\n",
    "    | deepseek_prompt\n",
    "    | llm\n",
    "    | MarkdownOutputParser()\n",
    "    | (lambda x: {\"reasoning\": x})\n",
    "    | exaone_prompt\n",
    "    | exaone_llm\n",
    "    | MarkdownOutputParser2()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb2b4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"양혜규의 신용할만한 산과 굴절에 대해 알려줘.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ef549de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### **🔹 모델 응답 결과**\n",
      "\n",
      "## 양혜규의 작품: 신용할 만한 산과 굴절 (#13, #14, #17, #22, #27, #28, #30)\n",
      "\n",
      "**개요:**\n",
      "\n",
      "양혜규의 작품 **신용할 만한 산과 굴절** 은 2010년 제작된 드로잉 시리즈입니다. 오스트리아 브레겐츠 미술관에서 2011년 개인전을 통해 처음 공개되었습니다. 이 작품은 **금융 정보를 담은 우편물 봉투** 를 재해석하여 제작되었습니다. \n",
      "\n",
      "**핵심 개념:**\n",
      "\n",
      "* **개념적 전환 (Détournement):**  일상적인 우편물 봉투에 숨겨진 의미를 드러내고 새로운 맥락을 부여합니다.\n",
      "* **소통과 정보:** 금융 정보를 담은 우편물을 통해 현대 사회에서 이루어지는 소통 방식과 정보의 흐름에 대한 질문을 던집니다.\n",
      "* **재활용과 예술:** 버려질 운명이었던 우편물 봉투를 예술 작품으로 재탄생시켜 물질의 새로운 가능성을 제시합니다.\n",
      "\n",
      "**구성:**\n",
      "\n",
      "* **개별 작품:**  <신용할 만한 산 #22>, <신용할 만한 산 #17>, <신용할 만한 산 #30>, <신용할 만한 굴절 #28>, <신용할 만한 굴절 #13>, <신용할 만한 굴절 #14>, <신용할 만한 굴절 #27> 로 구성됩니다. 각 작품은 번호로 구분되며, 전시장 공간과 상호작용하도록 설치됩니다.\n",
      "* **재료:** 카드보드에 보안 편지 봉투, 모눈종이, 콜라주가 주요 재료로 사용됩니다. 기하학적 패턴과 색상의 조화를 통해 시각적 효과를 창출합니다.\n",
      "\n",
      "**의미:**\n",
      "\n",
      "양혜규는 이 작품을 통해 다음과 같은 메시지를 전달합니다.\n",
      "\n",
      "* **일상 속 의미 찾기:** 평범한 물건 속에 숨겨진 의미와 가치를 발견하고 재해석하는 과정을 보여줍니다.\n",
      "* **물질의 순환:** 버려지는 물질에 예술적 생명력을 불어넣어 지속 가능한 사회에 대한 고찰을 제시합니다.\n",
      "* **정보 시대의 소통:** 금융 정보를 매개로 한 현대 사회의 소통 방식과 그 이면에 숨겨진 문제점을 질문합니다.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "501b6c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Content: 작품명: 신용할 만한 산과 굴절 #13, #14, #17, #22, #27, #28, #30 / N/A / Trustworthy Mountains and Refractions #13, #14, #17, #22, #27, #28, #30\n",
      "\n",
      "작가: 양혜규 / YANG Haegue\n",
      "\n",
      "작품 번호: 8088\n",
      "\n",
      "제작 연도: 2010\n",
      "\n",
      "크기: 99×69×(7)\n",
      "\n",
      "재료: 카드보드에 보안 편지 봉투, 모눈종이, 콜라주\n",
      "\n",
      "카테고리: 드로잉\n",
      "\n",
      "작품 설명: 양혜규(1971- )는 서울대 조소과를 졸업하고 독일 프랑크푸르트 국립학교 슈테델슐레에서 마이스터슐러 학위를 취득했다. 1994년이래 프랑크푸르트, 베를린, 서울을 기반으로 국제 미술무대에서 왕성하게 활동하며 대규모 설치, 조각, 평면 등 다양한 매체를 아우르는 작업을 선보여왔다.<신용할 만한 산과 굴절 #13, #14, #17, #22, #27, #28, #30>(2010)은 2011년 오스트리아 브레겐츠 미술관에서 선보였던 버전으로 <신용할 만한 산 #22>, <신용할 만한 산 #17>, <신용할 만한 산 #30>, <신용할 만한 굴절 #28>, <신용할 만한 굴절 #13>, <신용할 만한 굴절 #14>, <신용할 만한 굴절 #27>의 구성으로 금융 정보를 담은 우편물 봉투의 개념적 전환(détournement)에 집중한 초기 작업이라 할 수 있다. 은행 카드의 비밀번호 등, 금융 정보를 담은 우편물이 담겨 오는 편지 봉투의 내지를 주 재료로 한 꼴라쥬를 통해, 일상에서 보이는 혹은 가려진 소통에 대한 물음을 던진다. 꼴라주로 조형된 평면적인 이미지는 기하, 대칭, 그리고 색상의 단계적 차이 등 시각적 효과를 탐구한 결과물인 것이다. 작가는 산업 생산물인 편지 봉투의 수명 주기에 대해서 이야기 하고 있는데, 여기에는 편지 봉투의 이동 거리, 기능, 소재 및 다양한 디자인 패턴 등이 포함된다. 이제 작가는 이제껏 가려지거나 보호되었던 편지 봉투 안의 메시지들을 재조합함으로써, 편지 봉투의 단명적인 재료적 속성을 드러낸다. 그렇게 이미 사용된 후 존재 가치가 없는 것으로 여겨지는 공산품에 ‘제 2의 인생’을 부여한 것이다. 번호가 매겨진 제목을 지닌 낱개의 개별 작품들은 전시장의 건축 공간에 대응하며 작품 내부의 기하학적 패턴을 확장하는 방식으로 설치된다.\n",
      "Metadata: {'작품명': '신용할 만한 산과 굴절 #13, #14, #17, #22, #27, #28, #30 / N/A / Trustworthy Mountains and Refractions #13, #14, #17, #22, #27, #28, #30', '작가': '양혜규 / YANG Haegue', '제작 연도': '2010', '카테고리': '드로잉'}\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "Content: 작품명: 접힐 수 있는 것들의 체조 / N/A / Gymnastics of the Foldables\n",
      "\n",
      "작가: 양혜규 / YANG Haegue\n",
      "\n",
      "작품 번호: 6682\n",
      "\n",
      "제작 연도: 2006\n",
      "\n",
      "크기: 32.8×26.2×(15)\n",
      "\n",
      "재료: 흑백사진\n",
      "\n",
      "카테고리: 사진\n",
      "\n",
      "작품 설명: 양혜규는 최근 한국인 작가 중 가장 활발히 국제적인 무대에서 활동하는 작가라고 할 수 있다. 서울대 조소과와 프랑크푸르트 조형예술아카데미를 졸업한 후 주로 베를린에 머물면서 한국을 오가며 작품 활동을 하고 있는 그녀는 2006년 상파울로 비엔날레, 2008년 토리노 트리엔날레, 피츠버그, LA, 런던, 프랑크푸르트 등에서 전시를 가졌으며 2009년 베니스 비엔날레의 한국관 작가로 선정되면서 국제적 관심을 모았다.양혜규의 작업은 평범하고 소소한 일상적 경험과 기억에서 작품의 소재를 찾아 이를 개념적으로 재해석하는 것이다. 다양한 감각을 예술적 경험으로 치환시키는 그녀의 작업은 여성적인 감성과 사물을 바라보는 세밀한 시선을 특징으로 한다.이 작품은 소소한 일상적 경험에서 예술적 소재를 찾는 작가의 개념을 가장 간단하고도 명징하게 보여주는 작품으로 양혜규의 초기 사진작업 중 하나이다. 우리가 매일 접하는 빨래 걵보대를 여러 자세로 변형시켜 마치 움직이는 동작으로 체조를 하고 있는 것과 같은 형상의 연속을 만든다. 사진들은 매우 면밀하게 연출되었고 지극히 미니멀한 방식으로 세심하게 촬영되었다. 이러한 이미지는 약간의 유머와 함께 특별한 감정을 불러 일으키는데 그 감정은 결코 유쾌하지만은 않은 것이다. 다양한 감각을 예술적 경험으로 치환시키는 그녀의 작업은 여성적인 감성과 사물을 바라보는 세밀한 시선을 특징으로 한다.\n",
      "Metadata: {'작품명': '접힐 수 있는 것들의 체조 / N/A / Gymnastics of the Foldables', '작가': '양혜규 / YANG Haegue', '제작 연도': '2006', '카테고리': '사진'}\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "Content: 작품명: 여성형원주민-1.구변(口辯), 2.시골신기(神氣), 3.철지난 포화(飽和), 4.음력, 5.숙성, 6.상기된 결실 / N/A / Female Natives\n",
      "\n",
      "작가: 양혜규 / YANG Haegue\n",
      "\n",
      "작품 번호: 6681\n",
      "\n",
      "제작 연도: 2010\n",
      "\n",
      "크기: 1.185×106×106, 2.192×123×150, 3.197×103×103, 4.191×84×84, 5.235×105×105, 6.180×110×110\n",
      "\n",
      "재료: 옷걸이, 바퀴, 전구, 전선, 조화, 방울, 끈, 밧줄, 금속 체인, 털실, 말린 생강, 금속 고리, 알루미늄 반사기, 양철 깡통, 속이 빈 공, 조개 껍데기, 지점토, 말린 인삼, 채색한 목재 공, 화장 퍼프, 말린 목이버섯, 술, 헤어 롤, 금속 집게, 유리 비즈, 스팽글 패치\n",
      "\n",
      "카테고리: 조각ㆍ설치\n",
      "\n",
      "작품 설명: 양혜규는 최근 한국인 작가 중 가장 활발히 국제적인 무대에서 활동하는 작가라고 할 수 있다. 서울대 조소과와 프랑크푸르트 조형예술아카데미를 졸업한 후 주로 베를린에 머물면서 한국을 오가며 작품 활동을 하고 있는 그녀는 2006년 상파울로 비엔날레, 2008년 토리노 트리엔날레, 피츠버그, LA, 런던, 프랑크푸르트 등에서 전시를 가졌으며 2009년 베니스 비엔날레의 한국관 작가로 선정되면서 국제적 관심을 모았다.양혜규의 작업은 평범하고 소소한 일상적 경험과 기억에서 작품의 소재를 찾아 이를 개념적으로 재해석하는 것이다. 다양한 감각을 예술적 경험으로 치환시키는 그녀의 작업은 여성적인 감성과 사물을 바라보는 세밀한 시선을 특징으로 한다.이 작품은 주변의 일상적 사물들을 옷걸이에 걸고 그 위에 램프를 걸친 소위 '광원조각' 시리즈로 상당히 널리 알려지게 되었다. 이러한 빛 조각품은 각각이 마치 하나의 인격체처럼 보이고 이들은 서로 '군'을 이룬채 전시장에 널려 있다. 개인과 개인의 집합체로서의 사회가 지니는 관계를 은유적으로 보여주고 있는 이 작품들은 어쩔 수 없이 얽히고 속박되지만 각자의 빛을 지닌 인간의 삶에 대해 숙고하게 한다. 총 6개의 개별 존재가 한 작품을 이루는 <여성형 원주민-No.1 구변(口辯), No.2 시골신기(神氣), No.3 철지난 포화(飽和), No.4 음력, No.5 숙성, No.6 상기된 결실(Female Natives- No.1    Oratoricals, No.2 Possessed Hillbilly, No.3 Saturation out  ofSeason, No.4 Lunar Calendar, No.5 Maturing, No.6 Fruitful Glow)>은 광원 조각 시리즈 중 그 규모와 완성도가 매우 뛰어나다. 인조로 된 싸구려 꽃장식들이 화려하다기보다 오히려 애조를 자아내는 이 작품들은 브레겐츠 미술관의 개인전에 출품되어 호평을 받았다.\n",
      "Metadata: {'작품명': '여성형원주민-1.구변(口辯), 2.시골신기(神氣), 3.철지난 포화(飽和), 4.음력, 5.숙성, 6.상기된 결실 / N/A / Female Natives', '작가': '양혜규 / YANG Haegue', '제작 연도': '2010', '카테고리': '조각ㆍ설치'}\n",
      "--------------------------------------------------\n",
      "Document 4:\n",
      "Content: 작품명: 앉아있는 산 1 / N/A / A Mountain Sitting Cross Legged\n",
      "\n",
      "작가: 한애규 / HAHN Aikyu\n",
      "\n",
      "작품 번호: 6287\n",
      "\n",
      "제작 연도: 1992\n",
      "\n",
      "크기: 54×90×35\n",
      "\n",
      "재료: 백토조합토 1280\n",
      "\n",
      "카테고리: 공예\n",
      "\n",
      "작품 설명: 한애규(1953- )는 1994년 자신이 개인전에서 \"최근에 해온 나의 작업은 크게 두 부류로 나뉘는데 그 하나는 그때 그때 느낀 단상들을 그림 일기를 그리듯, 일기를 쓰듯 그려나간 것이고 다른 하나는 나의 여성에 관한 생각들을 부풀리고 각색하여 만든 것이다. 그리고 모든 것은 불합리하고, 모순되고, 병적이고, 납득할 수 없는 현실의 상황에 대한 '왜'라는 질문에서 시작한다.\" 라고 밝힌 바 있다.이와 같은 표현에서 알 수 있듯이 <앉아있는 산 1>(1992)은 여성을 마치 모신으로서 파악하여 사회의 구조적 억압 속에서 든든히 자신을 지켜내는 여성을 작품으로 표현한 것이다. '여성'과 '여성성'의 탐구를 통해 여성과 관련된 사회의 억압이나 구조적 모순을 일관되게 표현한 한애규의 작품은 페미니즘(Feminism) 미술을 연구하는데 도움이 될 것이다.\n",
      "Metadata: {'작품명': '앉아있는 산 1 / N/A / A Mountain Sitting Cross Legged', '작가': '한애규 / HAHN Aikyu', '제작 연도': '1992', '카테고리': '공예'}\n",
      "--------------------------------------------------\n",
      "Document 5:\n",
      "Content: 작품명: 기명절지도 / N/A / Still-Life with Vessels and Plants \n",
      "\n",
      "작가: 변성규 / BYUN Sungkyu\n",
      "\n",
      "작품 번호: 8406\n",
      "\n",
      "제작 연도: N/A\n",
      "\n",
      "크기: 104.3x25.6, 104.5x29.3, 104.5x29.1, 104x29.3, 104.3x27.5, 104.3x28, 104.3x27.1, 104x25.5\n",
      "\n",
      "재료: 종이에 먹, 색\n",
      "\n",
      "카테고리: 회화 I\n",
      "\n",
      "작품 설명: 석정(石亭) 변성규(卞成圭, 1890-1962)는 대구에서 한의사로 한의원을 경영하면서 취미로 그림을 배웠다. 그는 사군자를 포함하여 기명절지, 산수, 인물, 화조 등 여러 분야의 그림을 그렸다. 그는 서예에도 능해 경북 지역에 있는 정자의 현판을 많이 썼으며 그의 대표적인 현판으로는 ‘아양루(峨洋樓)’가 있다. 그의 작품은 현재 대구 지역에 다수 전하며 한국서예박물관 등에도 작품이 소장되어 있다. 그는 서양화가로 유명한 변종하(卞鐘夏, 1926-2000)의 부친이기도 하다.‘기명절지도(器皿折枝圖)’란 문인 취향의 여러 가지 그릇과 꽃가지, 과일, 괴석, 문방구 등의 소재들을 한 화면에 그린 것이며 주로 감상 및 장식 용도로 제작되었다. 이 작품은 세로로 긴 8폭의 화면에 매화, 연꽃, 대나무, 소나무 등 다양한 종류의 꽃가지로 꾸며진 화분과 청동기 등의 기물을 서로 엇갈리게 사선 구도로 배치하여 묘사한 작품으로, 화면 오른쪽에는 각각의 화면에 등장하는 꽃나무와 연관된 제시가 적혀 있다.[화제 풀이]◇ 제1폭似共東風別有因 봄바람을 함께 한 듯한 특별한 인연 있어絳羅高捲不勝春 붉은 비단옷 높이 말며 춘흥을 이기지 못하네.◇ 제2폭霜禽欲下先偸眼 겨울 철새가 날아가고 싶을 때는 먼저 (매화를) 훔쳐보고粉蝶如知合斷魂 분분히 나는 나비 (매화의 아름다움을) 알면 응당 넋을 잃으리.◇ 제3폭萬卷詩書眞眞活計 만권의 시문과 서화는 참으로 생활의 계책이요一盆梅竹自淸香 매화와 대 심긴 화분 하나는 스스로 맑은 향기 내 뿜는다.◇ 제4폭月缺霜濃細蘂發 달은 이지러지고 서리 짙더니 가는 꽃술이 피어났네.此花元屬桂堂仙 이 꽃은 원래 계수나무 집 살던 신선의 것이었다지.◇ 제5폭露濕紅芳複朶重 붉은 꽃 여러 송이 이슬에 거듭 젖어들고,風搖綠帶一枝長 푸른 띠 같은 기다란 가지를 바람이 흔든다.◇ 제6폭千紅百果皆零落 온갖 꽃과 과일들 죄 떨어지는데梧葉蕭蕭爾獨香 오동잎 쓸쓸하니 너 홀로 향기롭구나.◇ 제7폭雪白猩紅多別種 눈처럼 흰 것, 원숭이 같이 붉은 것, 종류가 많기도 하지만,也知黃色㝡居頭 그 중에서 황색 꽃을 가장 으뜸에 놓는다 알고 있다네.◇ 제8폭盆中偃蓋不知年 화분 속 누운 소나무 나이를 알지 못하는데半拂竹(?)半避烟 반쯤 떨친 대나무 ‘무엇’이 반쯤 연기를 피했구나.\n",
      "Metadata: {'작품명': '기명절지도 / N/A / Still-Life with Vessels and Plants', '작가': '변성규 / BYUN Sungkyu', '제작 연도': 'N/A', '카테고리': '회화 I'}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1464/3695306690.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content}\")  # 문서의 실제 내용\n",
    "    print(f\"Metadata: {doc.metadata}\")    # 메타데이터 (예: 출처, 페이지 등)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b13d46-a624-421e-80bd-f1f041d77b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 수행: 유사도 점수와 함께 반환\n",
    "docs_and_scores = retriever.vectorstore.similarity_search_with_score(query, k=5)\n",
    "\n",
    "# 검색된 문서 수 출력\n",
    "print(f\"검색된 문서 수: {len(docs_and_scores)}\")\n",
    "\n",
    "# 각 문서의 파일명, 전체 내용, 유사도 점수 출력\n",
    "for i, (doc, score) in enumerate(docs_and_scores, 1):\n",
    "    print(f\"\\n문서 {i}:\")\n",
    "    print(f\"  파일명: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"  유사도 점수: {score:.4f}\")\n",
    "    print(f\"  전체 내용: {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a46bc8",
   "metadata": {},
   "source": [
    "### PDF DB에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227476a-1804-4ac4-89dc-4197c5ed6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from langchain_core.documents import Document\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# 1. 임베딩 모델 초기화\n",
    "embedding_model = SentenceTransformer(\"nlpai-lab/KURE-v1\")\n",
    "\n",
    "# 2. FAISS 인덱스 로드 및 L2 거리 기반 인덱스 사용\n",
    "def load_existing_faiss_db(index_path, embedding_dim):\n",
    "    try:\n",
    "        faiss_index = faiss.read_index(index_path)  # 기존 FAISS DB 로드\n",
    "        print(f\"FAISS index loaded from {index_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index from {index_path}. Error: {e}\")\n",
    "        # L2 거리 기반 인덱스 새로 생성 (embedding_dim: 벡터 차원)\n",
    "        faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "        print(\"새로운 FAISS L2 인덱스 생성.\")\n",
    "    return faiss_index\n",
    "\n",
    "\n",
    "# 3. PDF에서 텍스트와 표를 문단별로 추출\n",
    "def extract_text_paragraphs(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 문서에서 문단별로 텍스트 추출 (표는 제외)\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    pdf_doc = fitz.open(pdf_path)  # PyMuPDF로 PDF 열기\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num in range(len(pdf_doc)):\n",
    "            page = pdf_doc[page_num]\n",
    "\n",
    "            # 텍스트 추출 (문단 단위로 구분)\n",
    "            text = page.get_text(\"text\")\n",
    "            paragraphs = text.split(\"\\n\\n\")  # 빈 줄로 문단 구분\n",
    "\n",
    "            # 표 추출\n",
    "            tables = pdf.pages[page_num].extract_tables()\n",
    "            table_texts = []\n",
    "            if tables:\n",
    "                for table in tables:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])  # 첫 행을 컬럼으로 설정\n",
    "                    table_texts.append(df.to_string())\n",
    "            else:\n",
    "                table_texts = []  # 표가 없는 경우 빈 리스트\n",
    "\n",
    "            # 문서 객체 생성 (문단별로 추가)\n",
    "            for para in paragraphs:\n",
    "                doc = Document(\n",
    "                    page_content=para + \"\\n\\n\" + \"\\n\\n\".join(table_texts),  # 문단 + 표 포함\n",
    "                    metadata={\"page\": page_num + 1}  # 페이지 정보 포함\n",
    "                )\n",
    "                doc_list.append(doc)\n",
    "\n",
    "    return doc_list\n",
    "\n",
    "# 4. 텍스트 벡터화 (SentenceTransformer 사용)\n",
    "def text_to_vector(text):\n",
    "    \"\"\"\n",
    "    텍스트를 벡터로 변환 (SentenceTransformer 사용)\n",
    "    \"\"\"\n",
    "    vector = embedding_model.encode(text)  # 바로 numpy 배열 반환\n",
    "    return vector\n",
    "\n",
    "\n",
    "# 5. FAISS DB에 새로운 문서 추가 (기존 인덱스 ID와 겹치지 않게 추가)\n",
    "def add_pdf_to_faiss(pdf_path, faiss_index, docstore, index_to_docstore_id):\n",
    "    \"\"\"\n",
    "    주어진 PDF에서 문단을 추출하고 FAISS 인덱스에 추가 (진행 상황 출력)\n",
    "    \"\"\"\n",
    "    # 기존 FAISS 인덱스에 추가된 벡터 수 파악 (새로운 벡터 ID가 기존과 겹치지 않도록 함)\n",
    "    existing_vector_count = faiss_index.ntotal\n",
    "    print(f\"기존 FAISS 인덱스의 벡터 수: {existing_vector_count}\")\n",
    "\n",
    "    # PDF에서 문단별로 텍스트 추출\n",
    "    documents = extract_text_paragraphs(pdf_path)\n",
    "    \n",
    "    # 문서의 총 문단 수\n",
    "    total_paragraphs = len(documents)\n",
    "    \n",
    "    # 문단 벡터화\n",
    "    vectors = []\n",
    "    for idx, doc in enumerate(documents):\n",
    "        # 진행 상황 출력\n",
    "        progress = (idx + 1) / total_paragraphs * 100\n",
    "        print(f\"진행 상황: {progress:.2f}% - {idx+1}/{total_paragraphs} 문단 처리 중...\")\n",
    "        \n",
    "        # 문단 벡터화\n",
    "        vector = text_to_vector(doc.page_content)\n",
    "        vectors.append(vector)\n",
    "\n",
    "        # 문서 ID와 문서 내용 저장 (docstore)\n",
    "        doc_id = existing_vector_count + len(vectors) - 1  # docstore에 새 문서의 ID는 마지막 인덱스 + 1\n",
    "        docstore[doc_id] = doc\n",
    "        index_to_docstore_id[existing_vector_count + idx] = str(doc_id)  # 벡터 ID와 문서 ID 연결\n",
    "\n",
    "    # 벡터를 FAISS DB에 추가\n",
    "    vectors = np.array(vectors).astype('float32')\n",
    "    \n",
    "    # 기존 FAISS 인덱스에 벡터 추가\n",
    "    faiss_index.add(vectors)\n",
    "\n",
    "    # FAISS DB 저장\n",
    "    faiss.write_index(faiss_index, \"./faiss_artworks_0303/index.faiss\")\n",
    "    print(f\"FAISS index updated and saved to './faiss_artworks_0303/index.faiss'.\")\n",
    "\n",
    "# 6. 전체 실행 흐름\n",
    "if __name__ == \"__main__\":\n",
    "    # 벡터 차원 크기 (SentenceTransformer 모델에 맞는 차원)\n",
    "    embedding_dim = 768  # 예시: KURE-v1 모델은 768차원\n",
    "\n",
    "    # 기존 FAISS DB 로드 (기존 인덱스가 없으면 새로 생성)\n",
    "    faiss_index = load_existing_faiss_db(\"./faiss_artworks_0303/index.faiss\", embedding_dim)\n",
    "\n",
    "    # 문서 저장소 초기화 (문서 ID와 내용 매핑)\n",
    "    docstore = {}\n",
    "    index_to_docstore_id = {i: i for i in range(len(docstore))}\n",
    "\n",
    "    # 새로운 PDF 파일 경로 지정\n",
    "    pdf_path = './PDF/dataset/artworks_10.pdf'\n",
    "\n",
    "    # 새 PDF 문서를 기존 FAISS DB에 추가\n",
    "    add_pdf_to_faiss(pdf_path, faiss_index, docstore, index_to_docstore_id)\n",
    "\n",
    "    # 새로운 문서가 docstore에 잘 추가되었는지 확인\n",
    "    print(f\"현재 문서 저장소에 저장된 문서 개수: {len(docstore)}\")\n",
    "    print(f\"첫 번째 문서 ID: {list(docstore.keys())[0]}\")\n",
    "    print(f\"첫 번째 문서 내용: {docstore[list(docstore.keys())[0]].page_content[:100]}...\")  # 첫 100글자만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ff200",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2317b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"문서 ID: {list(docstore.keys())[10]}\")\n",
    "print(f\"첫 번째 문서 내용: {docstore[list(docstore.keys())[10]].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"부산 전차 구조안전진단 및 보존처리에서 차량 구조는 어떻게 되었는가?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe29183",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"question\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62d908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매핑된 문서 ID 확인\n",
    "for vector_id in range(faiss_index.ntotal):\n",
    "    if vector_id in index_to_docstore_id:\n",
    "        doc_id = index_to_docstore_id[vector_id]\n",
    "        print(f\"벡터 ID: {vector_id}, 문서 ID: {doc_id}, 문서 내용: {docstore[doc_id].page_content[:100]}...\")\n",
    "    else:\n",
    "        print(f\"벡터 ID: {vector_id}에 대한 문서 ID가 매핑되지 않았습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be890d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_0217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
