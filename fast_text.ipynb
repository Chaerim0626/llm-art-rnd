{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìëª¨ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText ìëª¨ ë‹¨ìœ„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# ìëª¨ ë³€í™˜ëœ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip().split() for line in f.readlines()] \n",
    "\n",
    "# FastText ëª¨ë¸ í•™ìŠµ\n",
    "model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=10)\n",
    "\n",
    "# í•™ìŠµí•œ ëª¨ë¸ ì €ì¥\n",
    "model.save(\"fasttext_jamo.model\")\n",
    "print(\"FastText ìëª¨ ë‹¨ìœ„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = FastText.load(\"fasttext_jamo.model\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë‹¨ì–´ (ìëª¨ ë¶„ë¦¬)\n",
    "test_word = decompose_korean(\"ìì—°ì–´\")\n",
    "print(f\"ìëª¨ ë³€í™˜ëœ ë‹¨ì–´: {test_word}\")\n",
    "\n",
    "# ë‹¨ì–´ ë²¡í„° ì¶œë ¥\n",
    "print(f\"'{test_word}'ì˜ ë²¡í„° ê°’:\\n\", model.wv[test_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models import FastText\n",
    "from jamo import h2j, j2hcj\n",
    "import re\n",
    "\n",
    "# ìˆ«ì ë’¤ì— ë¶™ì€ í•œê¸€ì„ ìœ ì§€í•˜ë©´ì„œ ìˆ«ìëŠ” ë‹¨ë… í† í°ìœ¼ë¡œ ë³€í™˜\n",
    "def decompose_korean(text):\n",
    "    \"\"\"í•œê¸€ì€ ìëª¨ ë³€í™˜, ìˆ«ìëŠ” ë‹¨ë… í† í°ìœ¼ë¡œ ìœ ì§€í•˜ë˜ ìˆ«ì ë’¤ì˜ í•œê¸€ì€ ìœ ì§€\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        tokens = re.findall(r'\\d+|[^\\d\\s]+', text)  # ìˆ«ì + í•œê¸€ ë¶„ë¦¬ (ìˆ«ì ë’¤ í•œê¸€ í¬í•¨)\n",
    "        processed_tokens = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.isdigit():  # ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€\n",
    "                processed_tokens.append(token)\n",
    "                if i < len(tokens) - 1 and not tokens[i + 1].isdigit():  # ìˆ«ì ë’¤ í•œê¸€ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "                    processed_tokens.append(\"\".join(j2hcj(h2j(tokens[i + 1]))))\n",
    "            elif not token.isdigit() and (i == 0 or not tokens[i - 1].isdigit()):  # ìˆ«ìê°€ ì•„ë‹Œ í•œê¸€ë§Œ ë³€í™˜\n",
    "                processed_tokens.append(\"\".join(j2hcj(h2j(token))))\n",
    "\n",
    "        return \" \".join(processed_tokens)\n",
    "    return text\n",
    "\n",
    "# JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³€í™˜\n",
    "def load_json_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # ëª¨ë“  ë°ì´í„°ë¥¼ ìëª¨ ë³€í™˜ (ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€)\n",
    "    def recursive_decompose(obj):\n",
    "        if isinstance(obj, dict):  # ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬\n",
    "            return [recursive_decompose(value) for value in obj.values()]\n",
    "        elif isinstance(obj, list):  # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬\n",
    "            return [recursive_decompose(item) for item in obj]\n",
    "        elif isinstance(obj, str):  # ë¬¸ìì—´ ë³€í™˜\n",
    "            return decompose_korean(obj).split()  # ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    sentences = recursive_decompose(data)\n",
    "\n",
    "    # ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™” (ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ ì œê±°)\n",
    "    flat_sentences = [item for sublist in sentences for item in sublist if isinstance(sublist, list)]\n",
    "    return flat_sentences\n",
    "\n",
    "# JSON ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (í•œê¸€ì´ ìëª¨ ë³€í™˜ë˜ì§€ ì•Šì€ JSON)\n",
    "json_path = \"./output_jamo.json\"  # í•™ìŠµí•  JSON íŒŒì¼ ê²½ë¡œ\n",
    "sentences = load_json_data(json_path)\n",
    "\n",
    "# FastText ëª¨ë¸ í•™ìŠµ (ìˆ«ì í¬í•¨)\n",
    "model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=20)\n",
    "\n",
    "# í•™ìŠµí•œ ëª¨ë¸ ì €ì¥\n",
    "model_path = \"./fasttext_jamo_with_numbers.model\"\n",
    "model.save(model_path)\n",
    "print(f\"FastText ìëª¨ ë³€í™˜ + ìˆ«ì ìœ ì§€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ FastText í•™ìŠµ ë°ì´í„° (sentences) ìƒ˜í”Œ:\n",
      "1: ['ã…ˆã…ã„±ã„±ã…ã……ã…ã…ˆã…£ã„´']\n",
      "2: ['ä½œå®¶å¯«çœ']\n",
      "3: ['Selfportrait']\n",
      "4: ['ã…ã…ã„´ã„±ã…£ã……ã…“ã„±']\n",
      "5: ['HAN', 'Kisuk']\n",
      "6: []\n",
      "7: ['1960']\n",
      "8: ['41Ã—51']\n",
      "9: ['ã…ˆã…—ã…‡ã…‡ã…£ã…‡ã…”', 'ã…ˆã…”ã„¹ã„¹ã…ã…Œã…£ã„´ã……ã…£ã„¹ã…‚ã…“ã…ã…¡ã„¹ã…£ã„´ã…Œã…¡']\n",
      "10: ['ã……ã…ã…ˆã…£ã„´']\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ 10ê°œë§Œ ì¶œë ¥\n",
    "print(\"FastText í•™ìŠµ ë°ì´í„° (sentences) ìƒ˜í”Œ:\")\n",
    "for i, sentence in enumerate(sentences[:10]):  # ì• 10ê°œ ë¬¸ì¥ë§Œ ì¶œë ¥\n",
    "    print(f\"{i+1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ì´ 132348ê°œì˜ ë¬¸ì¥ì´ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n ì´ {len(sentences)}ê°œì˜ ë¬¸ì¥ì´ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from jamo import h2j, j2hcj\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# ë¬¸ì¥ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ê° ë‹¨ì–´ì˜ í‰ê·  ë²¡í„°)\n",
    "def sentence_to_vector(sentence, model):\n",
    "    tokens = decompose_korean(sentence)  # ìëª¨ ë³€í™˜\n",
    "    word_vectors = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            word_vectors.append(model.wv[token])  # FastText ë²¡í„° ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)  # ë¹ˆ ë¬¸ì¥ì¼ ê²½ìš° 0 ë²¡í„° ë°˜í™˜\n",
    "\n",
    "    return np.mean(word_vectors, axis=0)  # ëª¨ë“  ë‹¨ì–´ ë²¡í„° í‰ê·  ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ë¬¸ì¥ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "def calculate_similarity(sentence1, sentence2, model):\n",
    "    vec1 = sentence_to_vector(sentence1, model)\n",
    "    vec2 = sentence_to_vector(sentence2, model)\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ë¬¸ì¥ì—ì„œ ìˆ«ì ë¹„êµ ë° íŒ¨ë„í‹° ì ìš© í•¨ìˆ˜\n",
    "def compare_numbers(sentence1, sentence2):\n",
    "    \"\"\"ë‘ ë¬¸ì¥ì˜ ìˆ«ìë¥¼ ë¹„êµí•˜ê³  ë‹¤ë¥´ë©´ íŒ¨ë„í‹° ì ìš©\"\"\"\n",
    "    numbers1 = set(re.findall(r'\\d+', sentence1))  # ìˆ«ì ì¶”ì¶œ\n",
    "    numbers2 = set(re.findall(r'\\d+', sentence2))\n",
    "\n",
    "    if numbers1 and numbers2 and numbers1 != numbers2:  # ìˆ«ìê°€ ì¡´ì¬í•˜ì§€ë§Œ ë‹¤ë¥´ë©´\n",
    "        return 0.1  # íŒ¨ë„í‹° (ìœ ì‚¬ë„ë¥¼ ë‚®ì¶¤)\n",
    "    return 1.0  # ê·¸ëŒ€ë¡œ ìœ ì§€\n",
    "\n",
    "# ì»¤ìŠ¤í…€ FastText ìœ ì‚¬ë„ í•¨ìˆ˜ (ìˆ«ì ê³ ë ¤)\n",
    "def custom_similarity(text1, text2, model):\n",
    "    \"\"\"FastText ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°, ìˆ«ìê°€ ë‹¤ë¥´ë©´ íŒ¨ë„í‹° ì ìš©\"\"\"\n",
    "    tokens1 = text1.split()\n",
    "    tokens2 = text2.split()\n",
    "\n",
    "    # FastText í‰ê·  ë²¡í„° ê³„ì‚° (í•´ë‹¹ ë‹¨ì–´ê°€ ë²¡í„°ì— ìˆëŠ” ê²½ìš°ë§Œ)\n",
    "    vec1 = np.mean([model.wv[word] for word in tokens1 if word in model.wv], axis=0, keepdims=True)\n",
    "    vec2 = np.mean([model.wv[word] for word in tokens2 if word in model.wv], axis=0, keepdims=True)\n",
    "\n",
    "    if vec1.shape[0] == 0 or vec2.shape[0] == 0:\n",
    "        return 0.0  # í•œìª½ ë¬¸ì¥ì´ë¼ë„ FastTextì— ì—†ëŠ” ë‹¨ì–´ë§Œ ìˆìœ¼ë©´ ìœ ì‚¬ë„ 0\n",
    "\n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarity = np.dot(vec1, vec2.T) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    # ìˆ«ì ë¹„êµ í›„ íŒ¨ë„í‹° ì ìš©\n",
    "    penalty = compare_numbers(text1, text2)\n",
    "    final_score = similarity * penalty\n",
    "\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ RAG ì •ë‹µê³¼ LLM ì‘ë‹µì˜ ìœ ì‚¬ë„: 0.9837\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµëœ FastText ëª¨ë¸ ë¡œë“œ\n",
    "model = FastText.load(\"./fasttext_jamo_with_numbers.model\")\n",
    "\n",
    "# RAG ì •ë‹µê³¼ LLM ì‘ë‹µ ì˜ˆì œ\n",
    "rag_answer = \"\"\"\n",
    "ì´ì¤‘ì„­ì˜ í† ë¼í’€ ì‘í’ˆì€ 1941ë…„ì— ë§Œë“¤ì–´ì¡Œì–´ìš”.\n",
    "\"\"\"\n",
    "\n",
    "llm_response = \"\"\"\n",
    "ì´ì¤‘ì„­ì˜ í† ë¼í’€ ì‘í’ˆì˜ 1950ë…„ì— ë§Œë“¤ì–´ì¡Œì–´ìš”.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚°\n",
    "similarity = calculate_similarity(rag_answer, llm_response, model)\n",
    "\n",
    "print(f\"ğŸ”¹ RAG ì •ë‹µê³¼ LLM ì‘ë‹µì˜ ìœ ì‚¬ë„: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì ìˆ˜ë²”ìœ„ -1 ~ 1 \n",
    "# 1: ì™„ì „ ê°™ìŒ\n",
    "# 0: ì™„ì „ ë¬´ê´€\n",
    "# -1: ë°˜ëŒ€ë˜ëŠ” ì˜ë¯¸ì´ì§€ë§Œ ê±°ì˜ ë°œìƒ x\n",
    "\n",
    "# 0.7681, 0.2348, 0.8179, 0.8207, 0.7460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_0217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
