{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자모 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText 자모 단위 모델 학습 완료!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# 자모 변환된 한국어 문서를 불러오기\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip().split() for line in f.readlines()] \n",
    "\n",
    "# FastText 모델 학습\n",
    "model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=10)\n",
    "\n",
    "# 학습한 모델 저장\n",
    "model.save(\"fasttext_jamo.model\")\n",
    "print(\"FastText 자모 단위 모델 학습 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# 학습된 모델 불러오기\n",
    "model = FastText.load(\"fasttext_jamo.model\")\n",
    "\n",
    "# 테스트 단어 (자모 분리)\n",
    "test_word = decompose_korean(\"자연어\")\n",
    "print(f\"자모 변환된 단어: {test_word}\")\n",
    "\n",
    "# 단어 벡터 출력\n",
    "print(f\"'{test_word}'의 벡터 값:\\n\", model.wv[test_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models import FastText\n",
    "from jamo import h2j, j2hcj\n",
    "import re\n",
    "\n",
    "# 숫자 뒤에 붙은 한글을 유지하면서 숫자는 단독 토큰으로 변환\n",
    "def decompose_korean(text):\n",
    "    \"\"\"한글은 자모 변환, 숫자는 단독 토큰으로 유지하되 숫자 뒤의 한글은 유지\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        tokens = re.findall(r'\\d+|[^\\d\\s]+', text)  # 숫자 + 한글 분리 (숫자 뒤 한글 포함)\n",
    "        processed_tokens = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.isdigit():  # 숫자는 그대로 유지\n",
    "                processed_tokens.append(token)\n",
    "                if i < len(tokens) - 1 and not tokens[i + 1].isdigit():  # 숫자 뒤 한글이 있으면 추가\n",
    "                    processed_tokens.append(\"\".join(j2hcj(h2j(tokens[i + 1]))))\n",
    "            elif not token.isdigit() and (i == 0 or not tokens[i - 1].isdigit()):  # 숫자가 아닌 한글만 변환\n",
    "                processed_tokens.append(\"\".join(j2hcj(h2j(token))))\n",
    "\n",
    "        return \" \".join(processed_tokens)\n",
    "    return text\n",
    "\n",
    "# JSON 파일에서 데이터 불러오기 및 변환\n",
    "def load_json_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 모든 데이터를 자모 변환 (숫자는 그대로 유지)\n",
    "    def recursive_decompose(obj):\n",
    "        if isinstance(obj, dict):  # 딕셔너리 처리\n",
    "            return [recursive_decompose(value) for value in obj.values()]\n",
    "        elif isinstance(obj, list):  # 리스트 처리\n",
    "            return [recursive_decompose(item) for item in obj]\n",
    "        elif isinstance(obj, str):  # 문자열 변환\n",
    "            return decompose_korean(obj).split()  # 공백 기준으로 단어 분리\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    sentences = recursive_decompose(data)\n",
    "\n",
    "    # 리스트 평탄화 (중첩 리스트 제거)\n",
    "    flat_sentences = [item for sublist in sentences for item in sublist if isinstance(sublist, list)]\n",
    "    return flat_sentences\n",
    "\n",
    "# JSON 데이터 불러오기 (한글이 자모 변환되지 않은 JSON)\n",
    "json_path = \"./output_jamo.json\"  # 학습할 JSON 파일 경로\n",
    "sentences = load_json_data(json_path)\n",
    "\n",
    "# FastText 모델 학습 (숫자 포함)\n",
    "model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=20)\n",
    "\n",
    "# 학습한 모델 저장\n",
    "model_path = \"./fasttext_jamo_with_numbers.model\"\n",
    "model.save(model_path)\n",
    "print(f\"FastText 자모 변환 + 숫자 유지 모델 학습 완료! 저장 경로: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 FastText 학습 데이터 (sentences) 샘플:\n",
      "1: ['ㅈㅏㄱㄱㅏㅅㅏㅈㅣㄴ']\n",
      "2: ['作家寫眞']\n",
      "3: ['Selfportrait']\n",
      "4: ['ㅎㅏㄴㄱㅣㅅㅓㄱ']\n",
      "5: ['HAN', 'Kisuk']\n",
      "6: []\n",
      "7: ['1960']\n",
      "8: ['41×51']\n",
      "9: ['ㅈㅗㅇㅇㅣㅇㅔ', 'ㅈㅔㄹㄹㅏㅌㅣㄴㅅㅣㄹㅂㅓㅍㅡㄹㅣㄴㅌㅡ']\n",
      "10: ['ㅅㅏㅈㅣㄴ']\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 샘플 10개만 출력\n",
    "print(\"FastText 학습 데이터 (sentences) 샘플:\")\n",
    "for i, sentence in enumerate(sentences[:10]):  # 앞 10개 문장만 출력\n",
    "    print(f\"{i+1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 총 132348개의 문장이 학습 데이터로 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n 총 {len(sentences)}개의 문장이 학습 데이터로 사용됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from jamo import h2j, j2hcj\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# 문장 벡터를 생성하는 함수 (각 단어의 평균 벡터)\n",
    "def sentence_to_vector(sentence, model):\n",
    "    tokens = decompose_korean(sentence)  # 자모 변환\n",
    "    word_vectors = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            word_vectors.append(model.wv[token])  # FastText 벡터 가져오기\n",
    "\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)  # 빈 문장일 경우 0 벡터 반환\n",
    "\n",
    "    return np.mean(word_vectors, axis=0)  # 모든 단어 벡터 평균 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 문장의 코사인 유사도 계산\n",
    "def calculate_similarity(sentence1, sentence2, model):\n",
    "    vec1 = sentence_to_vector(sentence1, model)\n",
    "    vec2 = sentence_to_vector(sentence2, model)\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]  # 코사인 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 문장에서 숫자 비교 및 패널티 적용 함수\n",
    "def compare_numbers(sentence1, sentence2):\n",
    "    \"\"\"두 문장의 숫자를 비교하고 다르면 패널티 적용\"\"\"\n",
    "    numbers1 = set(re.findall(r'\\d+', sentence1))  # 숫자 추출\n",
    "    numbers2 = set(re.findall(r'\\d+', sentence2))\n",
    "\n",
    "    if numbers1 and numbers2 and numbers1 != numbers2:  # 숫자가 존재하지만 다르면\n",
    "        return 0.1  # 패널티 (유사도를 낮춤)\n",
    "    return 1.0  # 그대로 유지\n",
    "\n",
    "# 커스텀 FastText 유사도 함수 (숫자 고려)\n",
    "def custom_similarity(text1, text2, model):\n",
    "    \"\"\"FastText 기반 유사도 계산, 숫자가 다르면 패널티 적용\"\"\"\n",
    "    tokens1 = text1.split()\n",
    "    tokens2 = text2.split()\n",
    "\n",
    "    # FastText 평균 벡터 계산 (해당 단어가 벡터에 있는 경우만)\n",
    "    vec1 = np.mean([model.wv[word] for word in tokens1 if word in model.wv], axis=0, keepdims=True)\n",
    "    vec2 = np.mean([model.wv[word] for word in tokens2 if word in model.wv], axis=0, keepdims=True)\n",
    "\n",
    "    if vec1.shape[0] == 0 or vec2.shape[0] == 0:\n",
    "        return 0.0  # 한쪽 문장이라도 FastText에 없는 단어만 있으면 유사도 0\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity = np.dot(vec1, vec2.T) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    # 숫자 비교 후 패널티 적용\n",
    "    penalty = compare_numbers(text1, text2)\n",
    "    final_score = similarity * penalty\n",
    "\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 RAG 정답과 LLM 응답의 유사도: 0.9837\n"
     ]
    }
   ],
   "source": [
    "# 학습된 FastText 모델 로드\n",
    "model = FastText.load(\"./fasttext_jamo_with_numbers.model\")\n",
    "\n",
    "# RAG 정답과 LLM 응답 예제\n",
    "rag_answer = \"\"\"\n",
    "이중섭의 토끼풀 작품은 1941년에 만들어졌어요.\n",
    "\"\"\"\n",
    "\n",
    "llm_response = \"\"\"\n",
    "이중섭의 토끼풀 작품의 1950년에 만들어졌어요.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 유사도 계산\n",
    "similarity = calculate_similarity(rag_answer, llm_response, model)\n",
    "\n",
    "print(f\"🔹 RAG 정답과 LLM 응답의 유사도: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점수범위 -1 ~ 1 \n",
    "# 1: 완전 같음\n",
    "# 0: 완전 무관\n",
    "# -1: 반대되는 의미이지만 거의 발생 x\n",
    "\n",
    "# 0.7681, 0.2348, 0.8179, 0.8207, 0.7460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_0217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
