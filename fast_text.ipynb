{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³ ìœ ëª…ì‚¬ ì–´ì©Œì§€? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìëª¨ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText ìëª¨ ë‹¨ìœ„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# ìëª¨ ë³€í™˜ëœ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip().split() for line in f.readlines()] \n",
    "\n",
    "# FastText ëª¨ë¸ í•™ìŠµ\n",
    "model = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=10)\n",
    "\n",
    "# í•™ìŠµí•œ ëª¨ë¸ ì €ì¥\n",
    "model.save(\"fasttext_jamo.model\")\n",
    "print(\"FastText ìëª¨ ë‹¨ìœ„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = FastText.load(\"fasttext_jamo.model\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë‹¨ì–´ (ìëª¨ ë¶„ë¦¬)\n",
    "test_word = decompose_korean(\"ìì—°ì–´\")\n",
    "print(f\"ìëª¨ ë³€í™˜ëœ ë‹¨ì–´: {test_word}\")\n",
    "\n",
    "# ë‹¨ì–´ ë²¡í„° ì¶œë ¥\n",
    "print(f\"'{test_word}'ì˜ ë²¡í„° ê°’:\\n\", model.wv[test_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ë¬¸ì¥ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "def calculate_similarity(sentence1, sentence2, model):\n",
    "    vec1 = sentence_to_vector(sentence1, model)\n",
    "    vec2 = sentence_to_vector(sentence2, model)\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'ì‘ê°€ì‚¬ì§„', 'title_ch': 'ä½œå®¶å¯«çœ', 'title_eng': 'Selfportrait', 'artist': 'í•œê¸°ì„', 'artist_eng': 'HAN Kisuk', 'artwork_number': 1, 'year': '1960', 'size': '41Ã—51', 'materials': 'ì¢…ì´ì— ì ¤ë¼í‹´ì‹¤ë²„í”„ë¦°íŠ¸', 'category': 'ì‚¬ì§„', 'description': 'â€˜ë†(Nong)â€™ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë¯¸êµ­ì—ì„œ ë„ë¦¬ ì•Œë ¤ì§„ í•œë†(éŸ“è¾²) í•œê¸°ì„(1930-2011)ì€ êµ­ë‚´ í™œë™ì´ ê·¸ë¦¬ ë§ì§€ ì•Šì•„ì„œ í•œêµ­ í™”ë‹¨ì—ì„œëŠ” ìƒì†Œí•œ ì´ë¦„ì´ë‹¤. ê·¸ê°€ ìµœì´ˆë¡œ í•œêµ­ í™”ë‹¨ì— ë“±ì¥í•œ ê²ƒì€ 1971ë…„ 11ì›” ì‹ ì„¸ê³„ í™”ë‘ì—ì„œ ê°œìµœí•œã€ŠNong å±•ã€‹ì´í›„ì´ë‹¤. ê·¸ëŠ” ë†(Nong)ì„ êµ¬ë¦„ ìœ„ì˜ ì‹œì„ (è©©ä»™) í˜¹ì€ ì£¼ì„ (é…’ä»™)ê°™ì€ ì¡´ì¬ë¡œ ë¹„ìœ í•´ì„œ ë¯¸êµ­ì—ì„œ ìì‹ ì˜ ì´ë¦„ìœ¼ë¡œ ì“°ê³  ìˆë‹¤.ê·¸ì˜ ì‘í’ˆì€ ì „ë°˜ì ìœ¼ë¡œ ìì‹ ì˜ ì² í•™ì  ì´ë¯¸ì§€ë¥¼ ì¡°í˜•í™”ì‹œí‚¨ ì¶”ìƒ íšŒí™” ê³„í†µì´ë‹¤. ì¼ì¢…ì˜ í˜•ì´ìƒí•™ì ì¸ íšŒí™” í˜¹ì€ ì´ˆí˜„ì‹¤ì ì¸ í™˜ìƒì„¸ê³„ë¼ê³ ë„ í•  ìˆ˜ ìˆëŠ” ê·¸ì˜ ì‘í’ˆì€ ì–‘ì‹ì ì¸ ë©´ì—ì„œ ì£¼ë¡œ êµ¬ìƒì ì¸ í˜•íƒœë¥¼ ì·¨í•œë‹¤.í•œê¸°ì„ì˜ <ì‘ê°€ì‚¬ì§„>(1960)ì€ ë³¸ì¸ì˜ ì–¼êµ´ì„ ì°ì€ ê²ƒìœ¼ë¡œ, ì‚¬ì§„ ì†ì—ì„œ ì‘ê°€ëŠ” ìì‹ ì˜ ì‘í’ˆì„ ë°°ê²½ìœ¼ë¡œ í™”ë©´ì˜ ìš°ì¸¡ì„ ì£¼ì‹œí•˜ê³  ìˆë‹¤.', 'read_count': 10468}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open('json_data.json', 'r', encoding='utf-8') as f:\n",
    "    artwork_data = json.load(f)\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥\n",
    "print(artwork_data[0])  # ì²« ë²ˆì§¸ ì‘í’ˆ ì •ë³´ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText ìëª¨ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# FastText ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "    return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def get_embedding(sentence, model):\n",
    "    \"\"\"ë¬¸ì¥ ì„ë² ë”© ê³„ì‚° (FastText)\"\"\"\n",
    "    tokens = sentence.split()  # ë¬¸ì¥ì„ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”\n",
    "    vec = np.mean([model.wv[token] for token in tokens if token in model.wv], axis=0)  # í‰ê·  ë²¡í„° ê³„ì‚°\n",
    "    return vec\n",
    "\n",
    "# RAG ì •ë‹µê³¼ LLM ì‘ë‹µì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "def calculate_similarity(ground_truth, llm_response, model):\n",
    "    # RAG ì •ë‹µê³¼ LLM ì‘ë‹µì— ëŒ€í•œ ì„ë² ë”© ê³„ì‚°\n",
    "    vec1 = get_embedding(ground_truth, model)\n",
    "    vec2 = get_embedding(llm_response, model)\n",
    "    \n",
    "    # ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarity = cosine_similarity(vec1, vec2)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ì •ë‹µê³¼ LLM ì‘ë‹µì˜ ìœ ì‚¬ë„: 0.1882\n"
     ]
    }
   ],
   "source": [
    "# ì˜ˆì‹œ ë¬¸ì¥ë“¤ (ground_truthì™€ LLM ì‘ë‹µ)\n",
    "rag_answer = \"\"\"\n",
    "ì§‘ì—ê°€ê³ ì‹¶ì–´.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llm_response = \"\"\"\n",
    "ì§œì¦ì´ë‚˜.\n",
    "\"\"\"\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚°\n",
    "similarity = calculate_similarity(rag_answer, llm_response, model)\n",
    "\n",
    "print(f\"RAG ì •ë‹µê³¼ LLM ì‘ë‹µì˜ ìœ ì‚¬ë„: {similarity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì ì‹œë§Œ ì§ˆë¬¸ë„ ë­”ì§€ íŒŒì•…í•´ì•¼ë˜ëŠ”ê±°ì•„ëƒ? ì •ë‹µì´ë‘ llmë‹µë³€ë§Œ ë¹„êµí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì»¤ìŠ¤í…€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# âœ… FastText ëª¨ë¸ ë¡œë“œ\n",
    "model_path = \"./fasttext_jamo_with_numbers.model\"\n",
    "model = FastText.load(model_path)\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# âœ… ë¬¸ì¥ì„ FastText ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def get_sentence_embedding(sentence, model):\n",
    "    tokens = [token.form for token in kiwi.tokenize(sentence)]\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)  # ëª¨ë¸ì— ì—†ëŠ” ë‹¨ì–´ë§Œ ìˆìœ¼ë©´ 0 ë²¡í„° ë°˜í™˜\n",
    "    return np.mean(vectors, axis=0)  # í‰ê·  ë²¡í„° ë°˜í™˜\n",
    "\n",
    "# âœ… ìˆ«ì ë¹„êµ ë° íŒ¨ë„í‹° ì ìš© í•¨ìˆ˜\n",
    "def compare_numbers_ignore_order(sentence1, sentence2):\n",
    "    \"\"\"ìˆ«ìì˜ ê°’ì´ ë‹¤ë¥´ë©´ íŒ¨ë„í‹° ì ìš©, ìˆœì„œëŠ” ë¬´ì‹œ\"\"\"\n",
    "    numbers1 = set(re.findall(r'\\d+', sentence1))  # ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ ì§‘í•©ìœ¼ë¡œ ì €ì¥\n",
    "    numbers2 = set(re.findall(r'\\d+', sentence2))\n",
    "\n",
    "    if not numbers1 or not numbers2:\n",
    "        return 1.0  # ìˆ«ìê°€ ì—†ìœ¼ë©´ íŒ¨ë„í‹° ì—†ìŒ\n",
    "\n",
    "    if numbers1 != numbers2:  \n",
    "        return 0.1  # ìˆ«ì ê°’ì´ ë‹¤ë¥´ë©´ íŒ¨ë„í‹° ì ìš©\n",
    "\n",
    "    return 1.0  # ìˆ«ì ê°’ì´ ê°™ìœ¼ë©´ íŒ¨ë„í‹° ì—†ìŒ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë¬´ì‹œí•  ë‹¨ì–´ ëª©ë¡ (ì¡°ì‚¬, ì ‘ì†ì‚¬ ë“±)\n",
    "IGNORED_WORDS = {\"ì´\", \"ê·¸\", \"ê·¸ë¦¬ê³ \", \"í•˜ì§€ë§Œ\", \"ë˜í•œ\", \"ê·¸ëŸ¬ë‚˜\", \"ì¦‰\"}\n",
    "\n",
    "# âœ… ì¤‘ìš”í•œ ê°œë…ì„ ë‹´ê³  ìˆëŠ” í‚¤ì›Œë“œ ëª©ë¡\n",
    "KEYWORDS = {\"ì‘í’ˆ\", \"ì œì‘\", \"ì—°ë„\", \"ë…„ë„\", \"ë¯¸ìˆ \", \"ì˜ˆìˆ \", \"ì‘ê°€\", \"ì „ì‹œ\", \"ì†Œì¬\"}\n",
    "\n",
    "# âœ… í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ í•¨ìˆ˜ ê°œì„ \n",
    "def detect_hallucination(ground_truth, generated_text):\n",
    "    \"\"\"í•µì‹¬ ê°œë…ì´ ìœ ì§€ë˜ë©´ ê°ì í•˜ì§€ ì•Šê³ , ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€ ë˜ëŠ” ì •ë³´ ì‚­ì œ ì‹œ ê°ì \"\"\"\n",
    "    \n",
    "    # âœ… í˜•íƒœì†Œ ë¶„ì„ í›„ ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ë¹„êµ\n",
    "    gt_tokens = {token.form for token in kiwi.tokenize(ground_truth) if token.tag.startswith((\"N\", \"V\", \"X\"))}\n",
    "    gen_tokens = {token.form for token in kiwi.tokenize(generated_text) if token.tag.startswith((\"N\", \"V\", \"X\"))}\n",
    "\n",
    "    # âœ… ë¶ˆí•„ìš”í•œ ë‹¨ì–´(ì¡°ì‚¬, ì ‘ì†ì‚¬ ë“±) ì œê±°\n",
    "    gt_tokens -= IGNORED_WORDS\n",
    "    gen_tokens -= IGNORED_WORDS\n",
    "\n",
    "    # âœ… ì›ë˜ ìˆì–´ì•¼ í•  ë‹¨ì–´ ì¤‘ ì¤‘ìš”í•œ ê°œë… ë‹¨ì–´ë§Œ ì²´í¬\n",
    "    gt_key_tokens = gt_tokens & KEYWORDS\n",
    "    gen_key_tokens = gen_tokens & KEYWORDS\n",
    "\n",
    "    # âœ… í•µì‹¬ ê°œë…ì´ ìœ ì§€ë˜ì—ˆë‹¤ë©´ ê°ì í•˜ì§€ ì•ŠìŒ\n",
    "    if gt_key_tokens and gen_key_tokens and gt_key_tokens == gen_key_tokens:\n",
    "        return 1.0  # í•µì‹¬ ê°œë…ì´ ìœ ì§€ë˜ì—ˆìœ¼ë¯€ë¡œ ê°ì  X\n",
    "\n",
    "    # âœ… ìƒˆë¡œìš´ ê°œë…ì´ 2ê°œ ì´ìƒ ì¶”ê°€ë˜ë©´ ê°ì \n",
    "    extra_tokens = gen_tokens - gt_tokens  # ìƒì„±ëœ ë¬¸ì¥ì—ì„œ ì›ë¬¸ì— ì—†ëŠ” ë‹¨ì–´ ì°¾ê¸°\n",
    "    missing_tokens = gt_tokens - gen_tokens  # ì›ë˜ ë¬¸ì¥ì—ì„œ ë¹ ì§„ ë‹¨ì–´ ì°¾ê¸°\n",
    "\n",
    "    if len(extra_tokens) > 2:  # ìƒˆë¡œìš´ ì •ë³´ê°€ ì¶”ê°€ë˜ì—ˆì„ ë•Œ ê°ì \n",
    "        return 0.5\n",
    "\n",
    "    # âœ… ì¤‘ìš”í•œ ì •ë³´ê°€ ë¹ ì§€ë©´ ê°ì \n",
    "    if len(missing_tokens) > 2:\n",
    "        return 0.5\n",
    "\n",
    "    return 1.0  # í° ì°¨ì´ê°€ ì—†ìœ¼ë©´ ì ìˆ˜ ìœ ì§€\n",
    "\n",
    "\n",
    "def custom_similarity(text1, text2, model):\n",
    "    \"\"\"FastText ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°, ë™ì˜ì–´ ì ìš©, ìˆ«ì íŒ¨ë„í‹° ë°˜ì˜, í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€\"\"\"\n",
    "    # ë™ì˜ì–´ í™•ì¥ ì ìš©\n",
    "    # ë¬¸ì¥ ì„ë² ë”© ë³€í™˜ (FastText)\n",
    "    vec1 = get_sentence_embedding(text1, model)\n",
    "    vec2 = get_sentence_embedding(text2, model)\n",
    "\n",
    "    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
    "        return 0.0  # í•œìª½ ë¬¸ì¥ì´ë¼ë„ FastText ë²¡í„°ê°€ ì—†ìœ¼ë©´ ìœ ì‚¬ë„ 0\n",
    "\n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    # ìˆ«ì ë¹„êµ í›„ íŒ¨ë„í‹° ì ìš© (ìˆ«ì ìˆœì„œ ë¬´ì‹œ)\n",
    "    number_penalty = compare_numbers_ignore_order(text1, text2)\n",
    "\n",
    "    # í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ í›„ íŒ¨ë„í‹° ì ìš©\n",
    "    hallucination_penalty = detect_hallucination(text1, text2)\n",
    "\n",
    "    # ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚° (ìˆ«ì & í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨ë„í‹° ë°˜ì˜)\n",
    "    final_score = similarity.item() * number_penalty * hallucination_penalty  # numpy ê°’ -> float ë³€í™˜\n",
    "\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ìœ ì‚¬ë„ (ë™ì¼ ì˜ë¯¸ ë‹¤ë¥¸ í‘œí˜„): 0.9999\n",
      "ğŸ”¹ ìœ ì‚¬ë„ (ì˜ë¯¸ ìœ ì§€, ë‹¨ì–´ ìˆœì„œ ë‹¤ë¦„): 0.9495\n",
      "ğŸ”¹ ìœ ì‚¬ë„ (ìˆ«ì ë‹¤ë¦„): 0.0864\n",
      "ğŸ”¹ ìœ ì‚¬ë„ (ìˆ«ì ë‹¤ë¦„, ì¶”ê°€ ìˆ«ì ìˆìŒ): 0.0921\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì˜ˆì œ ë¬¸ì¥\n",
    "text1 = \"ì´ ì‘í’ˆì€ 1998ë…„ì— ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "text2 = \"ì´ ì‘í’ˆì€ 1998ë…„ì— ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.\"\n",
    "text3 = \"ì´ ì‘í’ˆì˜ ì œì‘ ì—°ë„ëŠ” 1998ë…„ì…ë‹ˆë‹¤.\"\n",
    "text4 = \"ì´ ì‘í’ˆì€ 1999ë…„ì— ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "text5 = \"ì´ ì‘í’ˆì€ 1998ë…„ì´ ì•„ë‹Œ 2000ë…„ì— ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# âœ… ìˆ«ì + ë¬¸ë§¥ + ë™ì˜ì–´ê¹Œì§€ ê³ ë ¤í•œ ìœ ì‚¬ë„ ë¹„êµ\n",
    "sim1 = custom_similarity(text1, text2, model)\n",
    "sim2 = custom_similarity(text1, text3, model)\n",
    "sim3 = custom_similarity(text1, text4, model)\n",
    "sim4 = custom_similarity(text1, text5, model)\n",
    "\n",
    "print(f\"ğŸ”¹ ìœ ì‚¬ë„ (ë™ì¼ ì˜ë¯¸ ë‹¤ë¥¸ í‘œí˜„): {sim1:.4f}\")  # ë†’ì€ ì ìˆ˜ ê¸°ëŒ€\n",
    "print(f\"ğŸ”¹ ìœ ì‚¬ë„ (ì˜ë¯¸ ìœ ì§€, ë‹¨ì–´ ìˆœì„œ ë‹¤ë¦„): {sim2:.4f}\")  # ë†’ì€ ì ìˆ˜ ê¸°ëŒ€\n",
    "print(f\"ğŸ”¹ ìœ ì‚¬ë„ (ìˆ«ì ë‹¤ë¦„): {sim3:.4f}\")  # ë‚®ì€ ì ìˆ˜ (íŒ¨ë„í‹° ì ìš©)\n",
    "print(f\"ğŸ”¹ ìœ ì‚¬ë„ (ìˆ«ì ë‹¤ë¦„, ì¶”ê°€ ìˆ«ì ìˆìŒ): {sim4:.4f}\")  # ë§¤ìš° ë‚®ì€ ì ìˆ˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ì •ë‹µê³¼ LLM ì‘ë‹µì˜ ìœ ì‚¬ë„: 0.4536\n"
     ]
    }
   ],
   "source": [
    "# RAG ì •ë‹µê³¼ LLM ì‘ë‹µ ì˜ˆì œ\n",
    "rag_answer = \"\"\"\n",
    "ê¹€ì„¸ì§„ì´ ì»´í“¨í„° ê·¸ë˜í”½ íšŒì‚¬ì—ì„œ ìµíŒ ë‹¤ì–‘í•œ ìµœì‹  ì˜ìƒ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ 'ë˜ëŒë ¤ì§„ ì‹œê°„'ì—ì„œ ì‹¤í—˜ì„±ê³¼ ë‹¹ëŒ€ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê°ê°ì ì¸ ë¹„ë””ì˜¤ ì‘í’ˆì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì´ ì‘í’ˆì€ ê°ê°ì˜ í™”ë©´ì—ëŠ” ìš¸ê¸°, ì„±ëƒ¥ ì¼œê¸°, ê·¸ë¦¬ê¸°, ë§í•˜ê¸°, ë¨¸ë¦¬ ë§ë¦¬ê¸°, ë¨¹ê¸° ë“± ì¼ìƒì ì¸ í–‰ìœ„ë¥¼ ì´¬ì˜í•œ í›„ì— ë¦¬ë²„ìŠ¤(reverse) ê¸°ë²•ìœ¼ë¡œ ë˜ëŒë¦° ì˜ìƒì´ ì¬ìƒë˜ë©°, \n",
    "\n",
    "í‰ë²”í•œ ìˆœê°„ì„ ë‹´ì€ ì˜ìƒì„ ë˜ëŒë ¸ì„ ë•Œì— ë°œìƒí•˜ëŠ” ë‚¯ì„  ê³µê°ê°ì  ë‚´ëŸ¬í‹°ë¸Œë¥¼ í†µí•´ ë˜ ë‹¤ë¥¸ ì°¨ì›ì˜ ì´ë¯¸ì§€ë¥¼ ì œì‹œí•˜ê³ ì í•œ ì‘ê°€ì˜ ì‹œë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llm_response = \"\"\"\n",
    "ê¹€ì„¸ì§„ì€ ì»´í“¨í„° ê·¸ë˜í”½ íšŒì‚¬ì—ì„œ ë‹¤ì–‘í•œ ìµœì‹  ì˜ìƒ ê¸°ë²•ì„ ìµí˜€ \"ë˜ëŒë ¤ì§„ ì‹œê°„\"ì—ì„œ ì´ëŸ¬í•œ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ 6ì±„ë„ ì˜ìƒì„ ì œì‘í–ˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì´ë¥¼ í†µí•´ ì‹œê°„ì˜ ë¬¼ë¦¬ì  íë¦„ì„ ë¦¬ë²„ìŠ¤í•˜ëŠ” ë³µì¡í•œ ì‹œê°ì  í‘œí˜„ì„ êµ¬í˜„í•˜ê³ , ë§¤ì²´ ì‹¤í—˜ì„±ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚°\n",
    "similarity = custom_similarity(rag_answer, llm_response, model)\n",
    "\n",
    "print(f\"RAG ì •ë‹µê³¼ LLM ì‘ë‹µì˜ ìœ ì‚¬ë„: {similarity:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì ìˆ˜ë²”ìœ„ -1 ~ 1 \n",
    "# 1: ì™„ì „ ê°™ìŒ\n",
    "# 0: ì™„ì „ ë¬´ê´€\n",
    "# -1: ë°˜ëŒ€ë˜ëŠ” ì˜ë¯¸ì´ì§€ë§Œ ê±°ì˜ ë°œìƒ x\n",
    "\n",
    "# 0.9612, 0.7238, 0.9073, 0.9053, 0.9434, 0.4258\n",
    "\n",
    "# ìˆ«ìê°€ ìˆì„ë•ŒëŠ” custom similartiyë¥¼ ì•„ë‹ˆë¼ë©´ calculate_similarityë¥¼ ì‚¬ìš© "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_0217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
